{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42dc7779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: imbalanced-learn library not found. SMOTE balancing will be skipped.\n",
      "Install it using: pip install imbalanced-learn\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 1: Imports and Initial Setup\n",
    "# =========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split # Used for train/val split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F # Needed for Attention softmax\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import warnings\n",
    "import copy # For saving best model state\n",
    "\n",
    "# Optional: SMOTE for class balancing\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    imblearn_available = True\n",
    "except ImportError:\n",
    "    print(\"Warning: imbalanced-learn library not found. SMOTE balancing will be skipped.\") # [cite: 338]\n",
    "    print(\"Install it using: pip install imbalanced-learn\") # [cite: 339]\n",
    "    imblearn_available = False\n",
    "\n",
    "# Optional: HMM for market regimes\n",
    "try:\n",
    "    from hmmlearn import hmm\n",
    "    hmmlearn_available = True\n",
    "except ImportError:\n",
    "    print(\"Warning: hmmlearn library not found. Market regime detection will be skipped.\")\n",
    "    hmmlearn_available = False\n",
    "\n",
    "warnings.filterwarnings('ignore') # [cite: 1, 339]\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42) # [cite: 339]\n",
    "torch.manual_seed(42) # [cite: 339]\n",
    "if torch.cuda.is_available(): # [cite: 339]\n",
    "    torch.cuda.manual_seed(42) # [cite: 339]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d02a3060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data loading process...\n",
      "Original data shape: (2433121, 13)\n",
      "\n",
      "First 5 rows of original data:\n",
      "             Timestamp     Open     High      Low    Close     Volume  \\\n",
      "0  2020-04-09 00:00:00  7311.27  7328.04  7311.27  7328.04   7.907567   \n",
      "1  2020-04-09 00:01:00  7324.97  7332.48  7323.85  7326.89   5.357228   \n",
      "2  2020-04-09 00:02:00  7329.96  7347.66  7329.96  7342.84  14.179910   \n",
      "3  2020-04-09 00:03:00  7342.84  7347.68  7339.99  7345.00   5.050320   \n",
      "4  2020-04-09 00:04:00  7347.59  7347.60  7334.80  7338.53   0.607997   \n",
      "\n",
      "                    datetime  start_time  blockheight  flow_mean  flow_total  \\\n",
      "0  2020-04-09 00:00:00+00:00         NaN          NaN        NaN         NaN   \n",
      "1  2020-04-09 00:01:00+00:00         NaN          NaN        NaN         NaN   \n",
      "2  2020-04-09 00:02:00+00:00         NaN          NaN        NaN         NaN   \n",
      "3  2020-04-09 00:03:00+00:00         NaN          NaN        NaN         NaN   \n",
      "4  2020-04-09 00:04:00+00:00         NaN          NaN        NaN         NaN   \n",
      "\n",
      "   transactions_count_flow  regime  \n",
      "0                      NaN     NaN  \n",
      "1                      NaN     NaN  \n",
      "2                      NaN     NaN  \n",
      "3                      NaN     NaN  \n",
      "4                      NaN     NaN  \n",
      "\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2433121 entries, 0 to 2433120\n",
      "Data columns (total 13 columns):\n",
      " #   Column                   Dtype  \n",
      "---  ------                   -----  \n",
      " 0   Timestamp                object \n",
      " 1   Open                     float64\n",
      " 2   High                     float64\n",
      " 3   Low                      float64\n",
      " 4   Close                    float64\n",
      " 5   Volume                   float64\n",
      " 6   datetime                 object \n",
      " 7   start_time               float64\n",
      " 8   blockheight              float64\n",
      " 9   flow_mean                float64\n",
      " 10  flow_total               float64\n",
      " 11  transactions_count_flow  float64\n",
      " 12  regime                   float64\n",
      "dtypes: float64(11), object(2)\n",
      "memory usage: 241.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 2: Load Data and Initial View\n",
    "# =========================================\n",
    "print(\"Starting data loading process...\") # [cite: 339]\n",
    "# <<< --- IMPORTANT: SET YOUR FILE PATH HERE --- >>>\n",
    "file_path = r\"C:\\Users\\User\\Downloads\\UMHackathon2025\\data\\merged_btc_price_flow_data.csv\" # Modified Placeholder - Original: [cite: 339]\n",
    "# <<< --- IMPORTANT: SET YOUR FILE PATH HERE --- >>>\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path) # [cite: 339]\n",
    "    print(f\"Original data shape: {df.shape}\") # [cite: 339]\n",
    "    print(\"\\nFirst 5 rows of original data:\") # [cite: 339]\n",
    "    print(df.head()) # [cite: 340]\n",
    "    print(\"\\nData Info:\") # [cite: 340]\n",
    "    df.info() # [cite: 340]\n",
    "except FileNotFoundError: # [cite: 340]\n",
    "    print(f\"Error: File not found at {file_path}\") # [cite: 340]\n",
    "    print(\"Please update the 'file_path' variable in the cell above.\") # [cite: 340]\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "38f8901b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing dataset to the last 200000 rows.\n",
      "Reduced data shape: (200000, 13)\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 3: Data Reduction (Optional)\n",
    "# =========================================\n",
    "# Keep this cell if you need to limit the dataset size, otherwise, you can remove it.\n",
    "num_rows_to_keep = 200000 # [cite: 340]\n",
    "if len(df) > num_rows_to_keep: # [cite: 340]\n",
    "    print(f\"Reducing dataset to the last {num_rows_to_keep} rows.\") # [cite: 340]\n",
    "    df = df.iloc[-num_rows_to_keep:] # [cite: 340]\n",
    "    print(f\"Reduced data shape: {df.shape}\") # [cite: 340]\n",
    "else:\n",
    "    print(\"Dataset size is within the limit, no reduction needed.\") # [cite: 340]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd48ea6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting timestamp and setting index...\n",
      "Timestamp converted and set as index.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 4: Timestamp Conversion and Indexing\n",
    "# =========================================\n",
    "print(\"Converting timestamp and setting index...\") # [cite: 341]\n",
    "# Adapt this based on your actual timestamp column name\n",
    "timestamp_col = 'Timestamp' # Assuming 'Timestamp' - check your CSV\n",
    "if timestamp_col not in df.columns:\n",
    "     # Try common alternatives or raise error\n",
    "     if 'datetime' in df.columns: timestamp_col = 'datetime'\n",
    "     elif 'Date' in df.columns: timestamp_col = 'Date'\n",
    "     else: raise KeyError(f\"Timestamp column ('{timestamp_col}' or alternatives) not found in DataFrame.\")\n",
    "\n",
    "df[timestamp_col] = pd.to_datetime(df[timestamp_col]) # [cite: 341]\n",
    "df.set_index(timestamp_col, inplace=True) # [cite: 341]\n",
    "df.sort_index(inplace=True) # Ensure data is chronologically sorted # [cite: 341]\n",
    "print(\"Timestamp converted and set as index.\") # [cite: 341]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88bbb2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the data...\n",
      "Forward filling NaNs in flow columns: ['flow_mean', 'flow_total', 'transactions_count_flow', 'regime']\n",
      "\n",
      "NaN count by column after initial cleaning (showing columns with NaNs):\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 5: Data Cleaning (Initial)\n",
    "# =========================================\n",
    "print(\"Cleaning the data...\") # [cite: 341]\n",
    "# Ensure standard OHLCV columns exist if possible\n",
    "ohlcv_map = {\n",
    "    'open_price': 'Open', 'high_price': 'High', 'low_price': 'Low',\n",
    "    'close_price': 'Close', 'volume': 'Volume',\n",
    "    'market_price_usd': 'Close' # Map market_price_usd if price_close doesn't exist [cite: 73, 79]\n",
    "}\n",
    "df.rename(columns=ohlcv_map, inplace=True)\n",
    "\n",
    "# Convert relevant columns to numeric\n",
    "numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume'] # Basic OHLCV\n",
    "# Add other known numeric cols from your specific merged file\n",
    "potential_numeric_cols = ['flow_mean', 'flow_total', 'transactions_count_flow'] # Add others as needed [cite: 341]\n",
    "\n",
    "for col in df.columns:\n",
    "    if col in numeric_cols or col in potential_numeric_cols:\n",
    "         if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "             try:\n",
    "                 df[col] = pd.to_numeric(df[col], errors='coerce') # [cite: 341]\n",
    "             except Exception as e:\n",
    "                 print(f\"Could not convert column {col} to numeric: {e}\") # [cite: 341]\n",
    "\n",
    "# Define flow columns based on *your actual data*\n",
    "flow_cols = ['flow_mean', 'flow_total', 'transactions_count_flow', 'regime'] # Example [cite: 341]\n",
    "existing_flow_cols = [col for col in flow_cols if col in df.columns] # [cite: 342]\n",
    "if existing_flow_cols:\n",
    "    print(f\"Forward filling NaNs in flow columns: {existing_flow_cols}\") # [cite: 342]\n",
    "    df[existing_flow_cols] = df[existing_flow_cols].fillna(method='ffill') # [cite: 342]\n",
    "    df[existing_flow_cols] = df[existing_flow_cols].fillna(0) # [cite: 342]\n",
    "\n",
    "# Fill missing OHLCV using ffill first, then 0 if needed (simple approach)\n",
    "ohlcv_cols_present = [col for col in ['Open', 'High', 'Low', 'Close', 'Volume'] if col in df.columns]\n",
    "df[ohlcv_cols_present] = df[ohlcv_cols_present].fillna(method='ffill').fillna(0)\n",
    "\n",
    "print(\"\\nNaN count by column after initial cleaning (showing columns with NaNs):\") # [cite: 342]\n",
    "nan_check = df.isna().sum() # [cite: 342]\n",
    "print(nan_check[nan_check > 0]) # [cite: 342]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "941cddfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering functions defined.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 6: Feature Engineering Functions\n",
    "# =========================================\n",
    "# --- Technical Indicator Functions (from improve.txt Cell 5.5 / CNN-LSTM.txt Cell 7/8) ---\n",
    "# (Keep your existing functions here for RSI, BB, Stoch, ATR etc.)\n",
    "\n",
    "def calculate_rsi(series, window=14): # [cite: 342]\n",
    "    \"\"\"Calculate Relative Strength Index (RSI).\"\"\"\n",
    "    delta = series.diff() # [cite: 342]\n",
    "    up = delta.clip(lower=0) # [cite: 343]\n",
    "    down = -1 * delta.clip(upper=0) # [cite: 343]\n",
    "    avg_gain = up.rolling(window=window, min_periods=1).mean() # Adjusted for rolling mean\n",
    "    avg_loss = down.rolling(window=window, min_periods=1).mean() # Adjusted for rolling mean\n",
    "    rs = avg_gain / avg_loss.replace(0, np.nan) # Avoid division by zero, replace with NaN [cite: 82]\n",
    "    rsi = 100 - (100 / (1 + rs)) # [cite: 343]\n",
    "    return rsi.fillna(50) # Fill initial NaNs with 50 (neutral)\n",
    "\n",
    "def calculate_bollinger_bands(series, window=20, nb_std=2): # [cite: 343]\n",
    "    \"\"\"Calculate Bollinger Bands.\"\"\"\n",
    "    middle_band = series.rolling(window=window, min_periods=1).mean() # [cite: 344]\n",
    "    std_dev = series.rolling(window=window, min_periods=1).std() # [cite: 344]\n",
    "    upper_band = middle_band + (std_dev * nb_std) # [cite: 344]\n",
    "    lower_band = middle_band - (std_dev * nb_std) # [cite: 344]\n",
    "    return upper_band, middle_band, lower_band\n",
    "\n",
    "def calculate_stochastic(high, low, close, k_window=14, d_window=3): # [cite: 344]\n",
    "    \"\"\"Calculate Stochastic Oscillator (%K and %D).\"\"\"\n",
    "    lowest_low = low.rolling(window=k_window, min_periods=1).min() # [cite: 345]\n",
    "    highest_high = high.rolling(window=k_window, min_periods=1).max() # [cite: 345]\n",
    "    denominator = highest_high - lowest_low # [cite: 345]\n",
    "    k = 100 * ((close - lowest_low) / denominator.replace(0, np.nan)) # Avoid division by zero [cite: 345]\n",
    "    d = k.rolling(window=d_window, min_periods=1).mean() # [cite: 345]\n",
    "    return k.fillna(50), d.fillna(50) # Fill initial NaNs\n",
    "\n",
    "def calculate_atr(high, low, close, window=14): # [cite: 346]\n",
    "    \"\"\"Calculate Average True Range (ATR).\"\"\"\n",
    "    tr1 = high - low # [cite: 346]\n",
    "    tr2 = abs(high - close.shift()) # [cite: 346]\n",
    "    tr3 = abs(low - close.shift()) # [cite: 346]\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1) # [cite: 347]\n",
    "    atr = tr.rolling(window=window, min_periods=1).mean() # [cite: 347]\n",
    "    return atr\n",
    "\n",
    "def calculate_macd(series, span1=12, span2=26, signal_span=9): # [cite: 83]\n",
    "    \"\"\"Calculate MACD, MACD Signal, and MACD Histogram.\"\"\"\n",
    "    ema1 = series.ewm(span=span1, adjust=False).mean() # [cite: 83, 356]\n",
    "    ema2 = series.ewm(span=span2, adjust=False).mean() # [cite: 83, 356]\n",
    "    macd = ema1 - ema2 # [cite: 83, 356]\n",
    "    macd_signal = macd.ewm(span=signal_span, adjust=False).mean() # [cite: 83, 356]\n",
    "    macd_hist = macd - macd_signal # [cite: 83]\n",
    "    return macd, macd_signal, macd_hist\n",
    "\n",
    "print(\"Feature engineering functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9108693f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying basic technical features...\n",
      "Basic technical features applied.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 7: Apply Basic Technical Features\n",
    "# =========================================\n",
    "print(\"Applying basic technical features...\")\n",
    "\n",
    "# Ensure we have a closing price column\n",
    "price_col = 'Close'\n",
    "if price_col not in df.columns:\n",
    "    raise ValueError(f\"Required column '{price_col}' not found after cleaning.\")\n",
    "\n",
    "df['Returns'] = df[price_col].pct_change() # [cite: 356]\n",
    "df['Log_Returns'] = np.log(df[price_col] / df[price_col].shift(1)) # Similar to [cite: 80]\n",
    "\n",
    "# Moving Averages\n",
    "windows = [5, 10, 20, 50, 200] # [cite: 81, 348, 356]\n",
    "for w in windows:\n",
    "    df[f'MA{w}'] = df[price_col].rolling(window=w, min_periods=1).mean() # [cite: 81, 348, 356]\n",
    "\n",
    "# MACD\n",
    "df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = calculate_macd(df[price_col]) # [cite: 83, 356]\n",
    "\n",
    "# RSI\n",
    "df['RSI'] = calculate_rsi(df[price_col], window=14) # [cite: 82, 357]\n",
    "\n",
    "# Bollinger Bands\n",
    "df['BB_Upper'], df['BB_Middle'], df['BB_Lower'] = calculate_bollinger_bands(df[price_col], window=20) # [cite: 357]\n",
    "df['BB_Width'] = (df['BB_Upper'] - df['BB_Lower']) / df['BB_Middle'].replace(0, np.nan) # Avoid division by zero [cite: 357]\n",
    "\n",
    "# Stochastic Oscillator\n",
    "if all(c in df.columns for c in ['High', 'Low']):\n",
    "    df['Stoch_K'], df['Stoch_D'] = calculate_stochastic(df['High'], df['Low'], df[price_col], k_window=14, d_window=3) # [cite: 357]\n",
    "else: print(\"Warning: High/Low columns missing, skipping Stochastic Oscillator.\")\n",
    "\n",
    "# ATR\n",
    "if all(c in df.columns for c in ['High', 'Low']):\n",
    "    df['ATR'] = calculate_atr(df['High'], df['Low'], df[price_col], window=14) # [cite: 357]\n",
    "else: print(\"Warning: High/Low columns missing, skipping ATR.\")\n",
    "\n",
    "print(\"Basic technical features applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "56007940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying volatility and trend features...\n",
      "Volatility and trend features applied.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 8: Apply Volatility and Trend Features\n",
    "# =========================================\n",
    "print(\"Applying volatility and trend features...\")\n",
    "\n",
    "# Volatility (using returns)\n",
    "vol_windows = [7, 14, 30, 60] # [cite: 80]\n",
    "for w in vol_windows:\n",
    "    df[f'Volatility_{w}d'] = df['Returns'].rolling(window=w, min_periods=1).std() * np.sqrt(w) # Scaled volatility\n",
    "\n",
    "# Moving Average Crossovers [cite: 81]\n",
    "if 'MA5' in df.columns and 'MA20' in df.columns: df['MA_Cross_5_20'] = df['MA5'] - df['MA20']\n",
    "if 'MA10' in df.columns and 'MA50' in df.columns: df['MA_Cross_10_50'] = df['MA10'] - df['MA50'] # Example\n",
    "if 'MA50' in df.columns and 'MA200' in df.columns: df['MA_Cross_50_200'] = df['MA50'] - df['MA200'] # [cite: 82]\n",
    "\n",
    "# Trend Direction based on long/short MAs [cite: 348]\n",
    "if 'MA50' in df.columns and 'MA200' in df.columns:\n",
    "     df['Trend_Direction'] = np.where(df['MA50'] > df['MA200'], 1,\n",
    "                                     np.where(df['MA50'] < df['MA200'], -1, 0)) # [cite: 348]\n",
    "     # Trend Change Detection\n",
    "     df['Trend_Change'] = df['Trend_Direction'].diff().fillna(0) # [cite: 349] adapted\n",
    "else: print(\"Warning: MA50 or MA200 missing, skipping Trend Direction/Change.\")\n",
    "\n",
    "print(\"Volatility and trend features applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "49de061f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying price/volume features...\n",
      "Price/volume features applied.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 9: Apply Price/Volume Interaction Features\n",
    "# =========================================\n",
    "print(\"Applying price/volume features...\")\n",
    "\n",
    "if 'Volume' in df.columns:\n",
    "    # Volume Change/Momentum\n",
    "    df['Volume_Change'] = df['Volume'].pct_change() # [cite: 358]\n",
    "    df['Volume_Momentum_5d'] = df['Volume'].pct_change(periods=5) # [cite: 353]\n",
    "\n",
    "    # Volume Moving Averages\n",
    "    vol_ma_windows = [5, 20, 50] # [cite: 353]\n",
    "    for w in vol_ma_windows:\n",
    "        df[f'Volume_MA{w}'] = df['Volume'].rolling(window=w, min_periods=1).mean() # [cite: 353]\n",
    "\n",
    "    # Volume relative to MA\n",
    "    if 'Volume_MA20' in df.columns:\n",
    "        df['Volume_Ratio'] = df['Volume'] / df['Volume_MA20'].replace(0, np.nan) # [cite: 353, 358]\n",
    "\n",
    "    # Price-Volume Product (simple interaction)\n",
    "    df['Price_Volume'] = df[price_col] * df['Volume'] # [cite: 358]\n",
    "\n",
    "    # On-Balance Volume (OBV)\n",
    "    obv = pd.Series(0, index=df.index) # [cite: 354]\n",
    "    price_diff = df[price_col].diff()\n",
    "    obv = np.where(price_diff > 0, df['Volume'], np.where(price_diff < 0, -df['Volume'], 0)).cumsum()\n",
    "    df['OBV'] = pd.Series(obv, index=df.index) # [cite: 354]\n",
    "\n",
    "else:\n",
    "    print(\"Warning: 'Volume' column not found. Skipping volume-related features.\")\n",
    "\n",
    "print(\"Price/volume features applied.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a034a071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining target variable...\n",
      "Target variable(s) defined.\n",
      "\n",
      "Columns after feature engineering: 51\n",
      "                        Open     High      Low    Close    Volume  \\\n",
      "Timestamp                                                           \n",
      "2024-07-08 06:01:00  54894.0  54994.0  54894.0  54994.0  0.068153   \n",
      "2024-07-08 06:02:00  55001.0  55097.0  55001.0  55097.0  9.472870   \n",
      "2024-07-08 06:03:00  55080.0  55163.0  55080.0  55163.0  2.334082   \n",
      "2024-07-08 06:04:00  55169.0  55171.0  55095.0  55106.0  1.701209   \n",
      "2024-07-08 06:05:00  55159.0  55162.0  55125.0  55125.0  0.188504   \n",
      "\n",
      "                                      datetime    start_time  blockheight  \\\n",
      "Timestamp                                                                   \n",
      "2024-07-08 06:01:00  2024-07-08 06:01:00+00:00  1.720418e+12     851212.0   \n",
      "2024-07-08 06:02:00  2024-07-08 06:02:00+00:00  1.720418e+12     851212.0   \n",
      "2024-07-08 06:03:00  2024-07-08 06:03:00+00:00  1.720418e+12     851212.0   \n",
      "2024-07-08 06:04:00  2024-07-08 06:04:00+00:00  1.720418e+12     851212.0   \n",
      "2024-07-08 06:05:00  2024-07-08 06:05:00+00:00  1.720418e+12     851212.0   \n",
      "\n",
      "                     flow_mean  flow_total  ...  Volume_MA5  Volume_MA20  \\\n",
      "Timestamp                                   ...                            \n",
      "2024-07-08 06:01:00   4.984456   29.906735  ...    0.068153     0.068153   \n",
      "2024-07-08 06:02:00   4.984456   29.906735  ...    4.770512     4.770512   \n",
      "2024-07-08 06:03:00   4.984456   29.906735  ...    3.958368     3.958368   \n",
      "2024-07-08 06:04:00   4.984456   29.906735  ...    3.394078     3.394078   \n",
      "2024-07-08 06:05:00   4.984456   29.906735  ...    2.752964     2.752964   \n",
      "\n",
      "                     Volume_MA50  Volume_Ratio   Price_Volume        OBV  \\\n",
      "Timestamp                                                                  \n",
      "2024-07-08 06:01:00     0.068153      1.000000    3748.008282   0.000000   \n",
      "2024-07-08 06:02:00     4.770512      1.985714  521926.718390   9.472870   \n",
      "2024-07-08 06:03:00     3.958368      0.589658  128754.974192  11.806952   \n",
      "2024-07-08 06:04:00     3.394078      0.501228   93746.808275  10.105743   \n",
      "2024-07-08 06:05:00     2.752964      0.068473   10391.297333  10.294248   \n",
      "\n",
      "                     Target_Return_1d  Target_Direction_1d  Target_Return_7d  \\\n",
      "Timestamp                                                                      \n",
      "2024-07-08 06:01:00          0.001873                    1         -0.001437   \n",
      "2024-07-08 06:02:00          0.001198                    1         -0.003739   \n",
      "2024-07-08 06:03:00         -0.001033                    0         -0.004460   \n",
      "2024-07-08 06:04:00          0.000345                    1         -0.001905   \n",
      "2024-07-08 06:05:00         -0.001977                    0         -0.003447   \n",
      "\n",
      "                     Target_Direction_7d  \n",
      "Timestamp                                 \n",
      "2024-07-08 06:01:00                    0  \n",
      "2024-07-08 06:02:00                    0  \n",
      "2024-07-08 06:03:00                    0  \n",
      "2024-07-08 06:04:00                    0  \n",
      "2024-07-08 06:05:00                    0  \n",
      "\n",
      "[5 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 10: Define Target Variable\n",
    "# =========================================\n",
    "print(\"Defining target variable...\")\n",
    "# Predicting direction 1 day ahead\n",
    "df['Target_Return_1d'] = df['Returns'].shift(-1) # [cite: 358] adapted\n",
    "df['Target_Direction_1d'] = np.where(df['Target_Return_1d'] > 0, 1, 0) # [cite: 358]\n",
    "\n",
    "# Optional: Predicting direction N days ahead (e.g., 7 days)\n",
    "days_ahead = 7\n",
    "if len(df) > days_ahead:\n",
    "    df[f'Target_Return_{days_ahead}d'] = df[price_col].shift(-days_ahead) / df[price_col] - 1 # [cite: 96]\n",
    "    df[f'Target_Direction_{days_ahead}d'] = np.where(df[f'Target_Return_{days_ahead}d'] > 0, 1, 0) # [cite: 96] adapted\n",
    "\n",
    "print(\"Target variable(s) defined.\")\n",
    "print(f\"\\nColumns after feature engineering: {len(df.columns)}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "07436c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying additional features (if columns exist)...\n",
      "Warning: Columns for NVT calculation not found.\n",
      "Warning: Columns for MVRV calculation not found.\n",
      "Warning: Columns for Whale Dominance calculation not found.\n",
      "Additional features applied.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 11: Feature Engineering - Add On-chain/Flow/Other Features (Optional)\n",
    "# =========================================\n",
    "print(\"Applying additional features (if columns exist)...\")\n",
    "\n",
    "# --- Add features inspired by CNN-LSTM.txt Cell 8 ---\n",
    "# --- Adapt these based on the *actual* columns available in your merged df ---\n",
    "\n",
    "# Example: NVT Ratio (adjust column names if needed)\n",
    "if 'market_cap_usd' in df.columns and 'transactions_count_flow' in df.columns: # Using transactions_count_flow as proxy for transaction rate/volume\n",
    "    # Need a measure of daily transaction value if possible, using count as rough proxy\n",
    "    df['nvt_proxy'] = df['market_cap_usd'] / df['transactions_count_flow'].replace(0, np.nan) #\n",
    "elif 'market_cap_usd' in df.columns and 'Volume' in df.columns: # Alternative proxy using trading volume\n",
    "     df['nvt_proxy'] = df['market_cap_usd'] / (df[price_col] * df['Volume']).rolling(window=7).mean().replace(0, np.nan) # Using 7d avg value traded\n",
    "else: print(\"Warning: Columns for NVT calculation not found.\")\n",
    "\n",
    "# Example: MVRV Ratio (adjust column names if needed)\n",
    "if 'market_cap_usd' in df.columns and 'realised_cap_usd' in df.columns: #\n",
    "    df['mvrv_ratio'] = df['market_cap_usd'] / df['realised_cap_usd'].replace(0, np.nan) #\n",
    "else: print(\"Warning: Columns for MVRV calculation not found.\")\n",
    "\n",
    "# Example: Flow Z-Score (adjust column names if needed)\n",
    "if 'flow_total' in df.columns: #\n",
    "    for window in [7, 14, 30]: #\n",
    "        flow_mean = df['flow_total'].rolling(window).mean() #\n",
    "        flow_std = df['flow_total'].rolling(window).std() #\n",
    "        df[f'flow_zscore_{window}d'] = (df['flow_total'] - flow_mean) / flow_std.replace(0, np.nan) #\n",
    "else: print(\"Warning: 'flow_total' column not found for Z-score.\")\n",
    "\n",
    "# Example: Whale Dominance (adjust column names if needed)\n",
    "balance_cols_map = { # Map potential names in your CSV to standard ones used in CNN-LSTM.txt\n",
    "    'addressesBalanceGreaterThan1k': 'addresses_with_1000_btc',\n",
    "    'addressesBalanceGreaterThan100': 'addresses_with_100_btc',\n",
    "    # ... add mappings for other balance columns if they exist in your data\n",
    "}\n",
    "df.rename(columns=balance_cols_map, inplace=True)\n",
    "required_balance_cols = ['addresses_with_1000_btc', 'addresses_with_100_btc'] # Minimum for basic whale metric\n",
    "if all(col in df.columns for col in required_balance_cols): #\n",
    "    whale_balance = df['addresses_with_1000_btc'] + df['addresses_with_100_btc'] #\n",
    "    # Attempt to find total address count or use a proxy\n",
    "    total_active_addresses = df['transactions_count_flow'] if 'transactions_count_flow' in df.columns else None # Very rough proxy\n",
    "    if total_active_addresses is not None:\n",
    "        df['whale_dominance_proxy'] = whale_balance / total_active_addresses.replace(0, np.nan) #\n",
    "    else:\n",
    "        # Use change in whale addresses as proxy if total count unavailable\n",
    "        df['whale_addr_change_7d'] = whale_balance.diff(periods=7) #\n",
    "else: print(\"Warning: Columns for Whale Dominance calculation not found.\")\n",
    "\n",
    "print(\"Additional features applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dfd8d90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling final NaN values after all feature engineering...\n",
      "NaN count before final filling: 173113\n",
      "Dropped 0 rows (primarily last rows with NaN target).\n",
      "\n",
      "Data shape after all preprocessing: (200000, 54)\n",
      "\n",
      "Confirmed: No NaNs remain in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 12: Final NaN Handling\n",
    "# =========================================\n",
    "print(\"Handling final NaN values after all feature engineering...\") #\n",
    "\n",
    "initial_nan_count = df.isna().sum().sum() #\n",
    "print(f\"NaN count before final filling: {initial_nan_count}\") #\n",
    "\n",
    "# Replace infinite values that might arise from calculations\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True) #\n",
    "\n",
    "# Use ffill first (most common for time series)\n",
    "df.fillna(method='ffill', inplace=True) #\n",
    "# Use bfill to fill any NaNs at the very beginning\n",
    "df.fillna(method='bfill', inplace=True) #\n",
    "# As a final safeguard, fill any remaining NaNs with 0 (e.g., if whole columns were NaN)\n",
    "df.fillna(0, inplace=True) #\n",
    "\n",
    "# Drop rows where the primary target is missing (e.g., the last few rows due to shift)\n",
    "target_column_name = 'Target_Direction_1d' # Choose your primary target\n",
    "initial_rows = len(df) #\n",
    "# Need to store the original target before filling potentially\n",
    "original_target = df[target_column_name].copy()\n",
    "# Re-calculate the target based on original price *before* filling, to avoid lookahead bias on fill\n",
    "# This step depends on how target was calculated. Assuming 'Returns' is reliable:\n",
    "# df['Target_Return_1d'] = df['Returns'].shift(-1) # Re-calculate before dropna\n",
    "# df[target_column_name] = np.where(df['Target_Return_1d'] > 0, 1, 0) # Re-calculate before dropna\n",
    "\n",
    "# Drop rows where target calculation resulted in NaN (typically last row(s))\n",
    "# It's often safer to drop based on the *source* of the target calculation before shift\n",
    "# E.g., drop the last row since 'Returns' shifted by -1 will be NaN there.\n",
    "if 'Target_Return_1d' in df.columns:\n",
    "     df.dropna(subset=['Target_Return_1d'], inplace=True) # Drop based on the shifted return being NaN\n",
    "else:\n",
    "     # Fallback: If target return wasn't kept, drop last row(s) == shift amount\n",
    "     shift_amount = 1 # For Target_Direction_1d\n",
    "     if len(df) > shift_amount:\n",
    "         df = df.iloc[:-shift_amount]\n",
    "\n",
    "\n",
    "final_rows = len(df) #\n",
    "print(f\"Dropped {initial_rows - final_rows} rows (primarily last rows with NaN target).\") #\n",
    "\n",
    "print(f\"\\nData shape after all preprocessing: {df.shape}\") #\n",
    "final_nan_count = df.isna().sum().sum() #\n",
    "if final_nan_count > 0: #\n",
    "     print(f\"\\nError: {final_nan_count} NaNs still remain after final filling!\") #\n",
    "     print(df.isna().sum()[df.isna().sum() > 0]) #\n",
    "else:\n",
    "     print(\"\\nConfirmed: No NaNs remain in the DataFrame.\") #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7c26738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final check on selected feature columns...\n",
      "\n",
      "Final features being used (40): ['Open', 'High', 'Low', 'Close', 'Volume', 'MA5', 'MA10', 'MA20', 'MA50', 'MA200', 'Volatility_7d', 'Volatility_14d', 'Volatility_30d', 'Volatility_60d', 'Returns', 'Log_Returns', 'MACD', 'MACD_Signal', 'MACD_Hist', 'RSI', 'BB_Width', 'Stoch_K', 'Stoch_D', 'ATR', 'MA_Cross_5_20', 'MA_Cross_10_50', 'MA_Cross_50_200', 'Trend_Direction', 'Trend_Change', 'Volume_Change', 'Volume_Momentum_5d', 'Volume_MA5', 'Volume_MA20', 'Volume_MA50', 'Volume_Ratio', 'Price_Volume', 'OBV', 'flow_zscore_7d', 'flow_zscore_14d', 'flow_zscore_30d']\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 13: Feature Selection\n",
    "# =========================================\n",
    "# Define core features you definitely want\n",
    "core_features = ['Open', 'High', 'Low', 'Close', 'Volume'] #\n",
    "\n",
    "# Define feature categories based on calculations above\n",
    "ma_features = [f'MA{w}' for w in windows]\n",
    "volatility_features = [f'Volatility_{w}d' for w in vol_windows]\n",
    "indicator_features = ['Returns', 'Log_Returns', 'MACD', 'MACD_Signal', 'MACD_Hist', 'RSI', 'BB_Width', 'Stoch_K', 'Stoch_D', 'ATR']\n",
    "trend_features = ['MA_Cross_5_20', 'MA_Cross_10_50', 'MA_Cross_50_200', 'Trend_Direction', 'Trend_Change']\n",
    "volume_features = ['Volume_Change', 'Volume_Momentum_5d', 'Volume_MA5', 'Volume_MA20', 'Volume_MA50', 'Volume_Ratio', 'Price_Volume', 'OBV']\n",
    "flow_features = [f'flow_zscore_{w}d' for w in [7, 14, 30]] + ['nvt_proxy', 'mvrv_ratio'] # Add other flow/onchain features calculated\n",
    "# Add other categories as needed (e.g., HODL waves if calculated)\n",
    "\n",
    "# Combine all potential features\n",
    "potential_features = core_features + ma_features + volatility_features + indicator_features + trend_features + volume_features + flow_features\n",
    "\n",
    "# Select only features that actually exist in the dataframe *after* cleaning\n",
    "feature_columns = [col for col in potential_features if col in df.columns] #\n",
    "\n",
    "# Final check for NaNs within selected features\n",
    "print(\"\\nFinal check on selected feature columns...\") #\n",
    "valid_feature_columns = [] #\n",
    "for col in feature_columns: #\n",
    "    if df[col].isna().any(): #\n",
    "        print(f\"Error: Column '{col}' selected contains NaN values AFTER final cleaning. Removing.\") #\n",
    "    else:\n",
    "        valid_feature_columns.append(col) #\n",
    "\n",
    "feature_columns = valid_feature_columns # Use only valid columns #\n",
    "\n",
    "# Define the target column to be used for prediction\n",
    "target_column_name = 'Target_Direction_1d' # Or 'Target_Direction_7d' etc.\n",
    "if target_column_name not in df.columns:\n",
    "    raise ValueError(f\"Target column '{target_column_name}' not found in DataFrame.\")\n",
    "\n",
    "print(f\"\\nFinal features being used ({len(feature_columns)}): {feature_columns}\") #\n",
    "if not feature_columns: #\n",
    "    raise ValueError(\"No valid feature columns remaining after checks!\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bd7a56f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (features) shape: (200000, 40)\n",
      "y (target) shape: (200000,)\n",
      "\n",
      "Target variable distribution:\n",
      "0    0.52843\n",
      "1    0.47157\n",
      "Name: proportion, dtype: float64\n",
      "X_scaled shape: (200000, 40)\n",
      "Scaling complete.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 14: Prepare X, y and Scale Features\n",
    "# =========================================\n",
    "X = df[feature_columns].values #\n",
    "y = df[target_column_name].values # Using the defined target column\n",
    "\n",
    "print(\"X (features) shape:\", X.shape) #\n",
    "print(\"y (target) shape:\", y.shape) #\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "print(pd.Series(y).value_counts(normalize=True))\n",
    "\n",
    "scaler = StandardScaler() #\n",
    "X_scaled = scaler.fit_transform(X) #\n",
    "print(\"X_scaled shape:\", X_scaled.shape) #\n",
    "print(\"Scaling complete.\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9b033ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total data points: 200000\n",
      "Final Training set size: 140000 samples (70.0%)\n",
      "Validation set size: 30000 samples (15.0%)\n",
      "Testing set size: 30000 samples (15.0%)\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 15: Train/Validation/Test Split (Chronological)\n",
    "# =========================================\n",
    "# Define split percentages\n",
    "train_percent = 0.7\n",
    "val_percent = 0.15\n",
    "test_percent = 0.15 # Ensure train + val + test = 1.0\n",
    "\n",
    "if not (train_percent + val_percent + test_percent == 1.0):\n",
    "    raise ValueError(\"Split percentages must sum to 1.0\")\n",
    "\n",
    "n = len(X_scaled)\n",
    "train_end_idx = int(n * train_percent)\n",
    "val_end_idx = train_end_idx + int(n * val_percent)\n",
    "\n",
    "X_train, y_train = X_scaled[:train_end_idx], y[:train_end_idx]\n",
    "X_val, y_val = X_scaled[train_end_idx:val_end_idx], y[train_end_idx:val_end_idx]\n",
    "X_test, y_test = X_scaled[val_end_idx:], y[val_end_idx:]\n",
    "\n",
    "# Store sizes for potential use later (e.g., aligning predictions)\n",
    "final_train_size = len(X_train) #\n",
    "val_size = len(X_val) #\n",
    "test_size = len(X_test) #\n",
    "\n",
    "print(f\"\\nTotal data points: {n}\") #\n",
    "print(f\"Final Training set size: {final_train_size} samples ({len(X_train)/n:.1%})\") #\n",
    "print(f\"Validation set size: {val_size} samples ({len(X_val)/n:.1%})\") #\n",
    "print(f\"Testing set size: {test_size} samples ({len(X_test)/n:.1%})\") #\n",
    "assert final_train_size + val_size + test_size == n, \"Split sizes error!\" #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "325a6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Cell 16: Sequence Creation Function\n",
    "# =========================================\n",
    "def create_sequences(X, y, time_steps=10): #\n",
    "    \"\"\"Creates sequences for time series forecasting.\"\"\"\n",
    "    Xs, ys = [], [] #\n",
    "    for i in range(len(X) - time_steps): #\n",
    "        Xs.append(X[i:(i + time_steps)]) #\n",
    "        ys.append(y[i + time_steps]) #\n",
    "    # Handle case where input length is less than time_steps\n",
    "    if not Xs: #\n",
    "        # Return empty arrays with correct dimensions if possible\n",
    "        n_features = X.shape[1] if X.ndim == 2 else 0 #\n",
    "        return np.empty((0, time_steps, n_features)), np.empty((0,)) #\n",
    "    return np.array(Xs), np.array(ys) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ff004d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training sequences shape: (139980, 20, 40), Target shape: (139980,)\n",
      "Validation sequences shape: (29980, 20, 40), Target shape: (29980,)\n",
      "Testing sequences shape: (29980, 20, 40), Target shape: (29980,)\n",
      "\n",
      "Input size (number of features) for model: 40\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 17: Apply Sequence Creation\n",
    "# =========================================\n",
    "time_steps = 20 # Define the lookback period (e.g., 20 days)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, time_steps) #\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, time_steps) #\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, time_steps) #\n",
    "\n",
    "print(f\"\\nTraining sequences shape: {X_train_seq.shape}, Target shape: {y_train_seq.shape}\") #\n",
    "print(f\"Validation sequences shape: {X_val_seq.shape}, Target shape: {y_val_seq.shape}\") #\n",
    "print(f\"Testing sequences shape: {X_test_seq.shape}, Target shape: {y_test_seq.shape}\") #\n",
    "\n",
    "if X_train_seq.shape[0] == 0: raise ValueError(\"Training sequence set is empty. Check data length and time_steps.\") #\n",
    "if X_val_seq.shape[0] == 0: print(\"\\nWarning: Validation sequence set is empty.\") #\n",
    "if X_test_seq.shape[0] == 0: print(\"\\nWarning: Test sequence set is empty.\") #\n",
    "\n",
    "input_size = X_train_seq.shape[2] # Should be number of features\n",
    "print(f\"\\nInput size (number of features) for model: {input_size}\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c67f20ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skipping SMOTE: 'apply_smote' is False.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 18: Apply SMOTE for Class Balancing (Optional)\n",
    "# =========================================\n",
    "apply_smote = False # <-- Set to True to try SMOTE balancing\n",
    "\n",
    "if apply_smote and imblearn_available and X_train_seq.shape[0] > 0: #\n",
    "    print(\"\\nAttempting SMOTE on training data...\") #\n",
    "    n_samples, n_timesteps, n_features = X_train_seq.shape #\n",
    "    # Reshape sequences for SMOTE (samples, features*timesteps)\n",
    "    X_train_flat = X_train_seq.reshape(n_samples, n_timesteps * n_features) #\n",
    "    y_train_flat = y_train_seq #\n",
    "\n",
    "    print(f\"Original training data shape (flattened): {X_train_flat.shape}\") #\n",
    "    print(f\"Original class distribution: {np.bincount(y_train_flat.astype(int))}\") #\n",
    "\n",
    "    # Ensure k_neighbors is less than the number of samples in the smallest class\n",
    "    min_class_count = np.min(np.bincount(y_train_flat.astype(int)))\n",
    "    k_neighbors = min(5, max(1, min_class_count - 1)) # Ensure k>=1 and k< minority samples\n",
    "\n",
    "    if k_neighbors < 1:\n",
    "        print(\"Warning: Not enough samples in the minority class for SMOTE. Skipping.\")\n",
    "        X_train_seq_final = X_train_seq\n",
    "        y_train_seq_final = y_train_seq\n",
    "        use_balanced_data = False\n",
    "    else:\n",
    "        print(f\"Using k_neighbors={k_neighbors} for SMOTE.\")\n",
    "        smote = SMOTE(random_state=42, k_neighbors=k_neighbors) #\n",
    "        try:\n",
    "            X_train_balanced_flat, y_train_balanced = smote.fit_resample(X_train_flat, y_train_flat) #\n",
    "\n",
    "            print(f\"Balanced training data shape (flattened): {X_train_balanced_flat.shape}\") #\n",
    "            print(f\"Balanced class distribution: {np.bincount(y_train_balanced.astype(int))}\") #\n",
    "\n",
    "            # Reshape back to sequences (samples, timesteps, features)\n",
    "            X_train_balanced_seq = X_train_balanced_flat.reshape(-1, n_timesteps, n_features) #\n",
    "\n",
    "            # Update sequence variables for DataLoader creation\n",
    "            X_train_seq_final = X_train_balanced_seq #\n",
    "            y_train_seq_final = y_train_balanced #\n",
    "            print(\"SMOTE applied successfully.\") #\n",
    "            use_balanced_data = True #\n",
    "        except ValueError as e: #\n",
    "            print(f\"SMOTE failed: {e}. Using original data.\") #\n",
    "            X_train_seq_final = X_train_seq #\n",
    "            y_train_seq_final = y_train_seq #\n",
    "            use_balanced_data = False #\n",
    "else:\n",
    "    if not imblearn_available and apply_smote: print(\"\\nSkipping SMOTE: imblearn not installed.\") #\n",
    "    elif not apply_smote: print(\"\\nSkipping SMOTE: 'apply_smote' is False.\") #\n",
    "    elif X_train_seq.shape[0] == 0: print(\"\\nSkipping SMOTE: No training data.\") #\n",
    "    else: print(\"\\nUsing original (unbalanced) training data.\")\n",
    "    X_train_seq_final = X_train_seq #\n",
    "    y_train_seq_final = y_train_seq #\n",
    "    use_balanced_data = False #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b773329c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor shapes:\n",
      "X_train_tensor: torch.Size([139980, 20, 40])\n",
      "y_train_tensor: torch.Size([139980, 1])\n",
      "X_val_tensor: torch.Size([29980, 20, 40])\n",
      "y_val_tensor: torch.Size([29980, 1])\n",
      "X_test_tensor: torch.Size([29980, 20, 40])\n",
      "y_test_tensor: torch.Size([29980, 1])\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 19: Convert to PyTorch Tensors\n",
    "# =========================================\n",
    "# Use the final (potentially balanced) training sequences\n",
    "X_train_tensor = torch.FloatTensor(X_train_seq_final) #\n",
    "y_train_tensor = torch.FloatTensor(y_train_seq_final).unsqueeze(1) # Add dimension for BCELoss\n",
    "\n",
    "# Use original validation and test sequences\n",
    "X_val_tensor = torch.FloatTensor(X_val_seq) #\n",
    "y_val_tensor = torch.FloatTensor(y_val_seq).unsqueeze(1) # Add dimension\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test_seq) #\n",
    "y_test_tensor = torch.FloatTensor(y_test_seq).unsqueeze(1) # Add dimension\n",
    "\n",
    "print(\"\\nTensor shapes:\") #\n",
    "print(f\"X_train_tensor: {X_train_tensor.shape}\") #\n",
    "print(f\"y_train_tensor: {y_train_tensor.shape}\") #\n",
    "print(f\"X_val_tensor: {X_val_tensor.shape}\") #\n",
    "print(f\"y_val_tensor: {y_val_tensor.shape}\") #\n",
    "print(f\"X_test_tensor: {X_test_tensor.shape}\") #\n",
    "print(f\"y_test_tensor: {y_test_tensor.shape}\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dd07e5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataLoaders created with batch size: 128\n",
      "Using original training data.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 20: Create DataLoaders\n",
    "# =========================================\n",
    "batch_size = 128 # Adjusted batch size\n",
    "\n",
    "# Training DataLoader (uses balanced data if SMOTE was applied and successful)\n",
    "if X_train_tensor.shape[0] > 0:\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor) #\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Shuffle training data\n",
    "else:\n",
    "    train_loader = None\n",
    "    print(\"Warning: Training data is empty, train_loader not created.\")\n",
    "\n",
    "# Validation DataLoader\n",
    "if X_val_tensor.shape[0] > 0: #\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor) #\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) # No shuffle for validation\n",
    "else:\n",
    "    val_loader = None #\n",
    "    print(\"Validation data is empty, val_loader not created.\") #\n",
    "\n",
    "# Testing DataLoader\n",
    "if X_test_tensor.shape[0] > 0:\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor) #\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # No shuffle for testing\n",
    "else:\n",
    "    test_loader = None\n",
    "    print(\"Warning: Test data is empty, test_loader not created.\") #\n",
    "\n",
    "print(f\"\\nDataLoaders created with batch size: {batch_size}\") #\n",
    "print(f\"Using {'balanced' if use_balanced_data else 'original'} training data.\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9658b7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTMModel class defined.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 21: CNN-LSTM Model Definition\n",
    "# =========================================\n",
    "# Based on improve.txt Cell 15 (Modified)\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size,\n",
    "                 dropout_prob=0.3, num_cnn_layers=2, cnn_filters=None, cnn_kernel_size=3):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size #\n",
    "        self.num_layers = num_layers #\n",
    "\n",
    "        # --- CNN Layers ---\n",
    "        if cnn_filters is None: #\n",
    "            cnn_filters = [32, 64] # Default filters\n",
    "        if len(cnn_filters) != num_cnn_layers: #\n",
    "            raise ValueError(f\"Expected {num_cnn_layers} CNN filter values, got {len(cnn_filters)}\") #\n",
    "\n",
    "        self.cnn_layers = nn.ModuleList() #\n",
    "        in_channels = input_size  # Input features are treated as channels for Conv1d #\n",
    "        cnn_output_channels = in_channels # Initialize in case num_cnn_layers is 0\n",
    "\n",
    "        for i in range(num_cnn_layers): #\n",
    "            out_channels = cnn_filters[i] #\n",
    "            # Calculate padding to maintain sequence length: padding = (kernel_size - 1) // 2\n",
    "            padding = (cnn_kernel_size - 1) // 2\n",
    "            self.cnn_layers.append(nn.Sequential( #\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=cnn_kernel_size, padding=padding), #\n",
    "                nn.ReLU(), #\n",
    "                nn.BatchNorm1d(out_channels), # Add BatchNorm\n",
    "                # Optional: Add Dropout within CNN\n",
    "                # nn.Dropout(dropout_prob * 0.5) # Smaller dropout for CNN layers\n",
    "            )) #\n",
    "            in_channels = out_channels # Output channels of this layer is input for the next #\n",
    "            cnn_output_channels = out_channels # Store the last output channels\n",
    "\n",
    "        # --- LSTM Layer ---\n",
    "        # Input size for LSTM is the number of output channels from the last CNN layer\n",
    "        lstm_input_size = cnn_output_channels #\n",
    "        self.lstm = nn.LSTM(lstm_input_size, hidden_size, num_layers, #\n",
    "                            batch_first=True, dropout=dropout_prob if num_layers > 1 else 0, #\n",
    "                            bidirectional=True) # Using Bidirectional LSTM #\n",
    "\n",
    "        # --- Attention Layer ---\n",
    "        # Input to attention is the output of LSTM (hidden_size * 2 because bidirectional)\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1) #\n",
    "\n",
    "        # --- Fully Connected Layers ---\n",
    "        # Input to FC layers is the context vector from attention (hidden_size * 2)\n",
    "        fc_input_size = hidden_size * 2\n",
    "        self.bn_fc = nn.BatchNorm1d(fc_input_size) # BatchNorm before FC layers #\n",
    "        self.fc1 = nn.Linear(fc_input_size, hidden_size) #\n",
    "        self.relu1 = nn.ReLU() #\n",
    "        self.dropout1 = nn.Dropout(dropout_prob) #\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2) #\n",
    "        self.relu2 = nn.ReLU() #\n",
    "        self.dropout2 = nn.Dropout(dropout_prob) #\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, output_size) #\n",
    "        self.sigmoid = nn.Sigmoid() # Use Sigmoid for BCELoss #\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x shape: [batch_size, seq_len, input_size]\n",
    "\n",
    "        # --- CNN Forward ---\n",
    "        # Conv1d expects input as [batch_size, channels=input_size, seq_len]\n",
    "        x_cnn = x.transpose(1, 2) # Transpose to fit Conv1d #\n",
    "        for cnn_layer in self.cnn_layers: #\n",
    "            x_cnn = cnn_layer(x_cnn) #\n",
    "        # Output x_cnn shape: [batch_size, cnn_output_channels, seq_len]\n",
    "\n",
    "        # --- LSTM Forward ---\n",
    "        # LSTM expects input as [batch_size, seq_len, features=cnn_output_channels]\n",
    "        x_lstm = x_cnn.transpose(1, 2) # Transpose back #\n",
    "\n",
    "        # Initialize hidden and cell states (for bidirectional)\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device) # *2 for bidirectional #\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device) # *2 for bidirectional #\n",
    "        lstm_out, _ = self.lstm(x_lstm, (h0, c0)) #\n",
    "        # lstm_out shape: [batch_size, seq_len, hidden_size * 2]\n",
    "\n",
    "        # --- Attention Forward ---\n",
    "        # Calculate attention weights\n",
    "        attn_weights = F.softmax(self.attention(lstm_out), dim=1) # Apply softmax over sequence length dim #\n",
    "        # Calculate context vector by weighted sum\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1) # Shape: [batch_size, hidden_size * 2] #\n",
    "\n",
    "        # --- Fully Connected Forward ---\n",
    "        context_vector = self.bn_fc(context_vector) # Apply BatchNorm #\n",
    "        out = self.fc1(context_vector) #\n",
    "        out = self.relu1(out) #\n",
    "        out = self.dropout1(out) #\n",
    "        out = self.fc2(out) #\n",
    "        out = self.relu2(out) #\n",
    "        out = self.dropout2(out) #\n",
    "        out = self.fc3(out) #\n",
    "        out = self.sigmoid(out) # Apply sigmoid for probability output #\n",
    "\n",
    "        return out #\n",
    "\n",
    "print(\"CNNLSTMModel class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2af61bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "CNN-LSTM Model Architecture:\n",
      "CNNLSTMModel(\n",
      "  (cnn_layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv1d(40, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(64, 64, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (attention): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (bn_fc): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (dropout2): Dropout(p=0.3, inplace=False)\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Loss: BCELoss()\n",
      "Optimizer: Adam(lr=0.001)\n",
      "Scheduler: ReduceLROnPlateau(patience=5, factor=0.1)\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 22: Initialize CNN-LSTM Model, Loss, Optimizer, Scheduler\n",
    "# =========================================\n",
    "# Model Hyperparameters\n",
    "hidden_size = 64         # LSTM hidden units\n",
    "num_lstm_layers = 2      # Number of LSTM layers\n",
    "output_size = 1          # Binary classification (0 or 1)\n",
    "dropout_prob = 0.3       # Dropout probability\n",
    "\n",
    "# CNN Hyperparameters\n",
    "num_cnn_layers = 2       # Number of Conv1D layers\n",
    "cnn_filters = [32, 64]   # Filters for each Conv1D layer (e.g., 2 layers: [32, 64])\n",
    "cnn_kernel_size = 3      # Kernel size for Conv1D\n",
    "\n",
    "# Training Hyperparameters\n",
    "learning_rate = 0.001\n",
    "scheduler_patience = 5\n",
    "scheduler_factor = 0.1\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #\n",
    "print(f\"Using device: {device}\") #\n",
    "\n",
    "# Ensure input_size is defined from Cell 17\n",
    "if 'input_size' not in locals():\n",
    "     raise NameError(\"Variable 'input_size' not defined. Ensure Cell 17 ran successfully.\")\n",
    "\n",
    "# Initialize the CNN-LSTM model\n",
    "cnn_lstm_model = CNNLSTMModel(\n",
    "    input_size=input_size, # From Cell 17\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_lstm_layers,\n",
    "    output_size=output_size,\n",
    "    dropout_prob=dropout_prob,\n",
    "    num_cnn_layers=num_cnn_layers,\n",
    "    cnn_filters=cnn_filters,\n",
    "    cnn_kernel_size=cnn_kernel_size\n",
    ").to(device) #\n",
    "\n",
    "print(\"\\nCNN-LSTM Model Architecture:\") #\n",
    "print(cnn_lstm_model) #\n",
    "\n",
    "# Loss Function (Binary Cross Entropy for binary classification)\n",
    "criterion = nn.BCELoss() #\n",
    "\n",
    "# Optimizer (Adam is a common choice)\n",
    "optimizer = optim.Adam(cnn_lstm_model.parameters(), lr=learning_rate) #\n",
    "\n",
    "# Learning Rate Scheduler (Reduce LR on plateau)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=scheduler_factor, patience=scheduler_patience, verbose=True #\n",
    ")\n",
    "\n",
    "print(f\"\\nLoss: {criterion}\") #\n",
    "print(f\"Optimizer: Adam(lr={learning_rate})\") #\n",
    "print(f\"Scheduler: ReduceLROnPlateau(patience={scheduler_patience}, factor={scheduler_factor})\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5a44573e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function 'train_model_with_validation' defined.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 23: Training Function Definition\n",
    "# =========================================\n",
    "# Use the function from improve.txt Cell 17/18\n",
    "\n",
    "def train_model_with_validation(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, patience=5, max_grad_norm=1.0): #\n",
    "    \"\"\"Trains the model with validation loop, early stopping, and LR scheduling.\"\"\"\n",
    "    train_losses, train_accuracies = [], [] #\n",
    "    val_losses, val_accuracies = [], [] #\n",
    "    best_val_loss = float('inf') #\n",
    "    epochs_no_improve = 0 #\n",
    "    best_model_state = None #\n",
    "    early_stop_triggered = False #\n",
    "\n",
    "    print(\"\\n--- Starting Model Training ---\") #\n",
    "    for epoch in range(num_epochs): #\n",
    "        # --- Training Phase ---\n",
    "        model.train() # Set model to training mode\n",
    "        running_loss, correct_predictions, total_samples = 0.0, 0, 0 #\n",
    "        for inputs, labels in train_loader: #\n",
    "            inputs, labels = inputs.to(device), labels.to(device) #\n",
    "\n",
    "            optimizer.zero_grad() # Zero gradients #\n",
    "            outputs = model(inputs) # Forward pass #\n",
    "            loss = criterion(outputs, labels) # Calculate loss #\n",
    "            loss.backward() # Backward pass #\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm) # Gradient Clipping #\n",
    "            optimizer.step() # Update weights #\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0) # Accumulate loss #\n",
    "            predicted = (outputs > 0.5).float() # Convert probs to 0/1 predictions #\n",
    "            correct_predictions += (predicted == labels).sum().item() # Count correct #\n",
    "            total_samples += labels.size(0) # Count total samples #\n",
    "        epoch_train_loss = running_loss / total_samples if total_samples > 0 else 0 #\n",
    "        epoch_train_acc = correct_predictions / total_samples if total_samples > 0 else 0 #\n",
    "        train_losses.append(epoch_train_loss) #\n",
    "        train_accuracies.append(epoch_train_acc) #\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        epoch_val_loss, epoch_val_acc = None, None #\n",
    "        if val_loader: #\n",
    "            model.eval() # Set model to evaluation mode\n",
    "            val_running_loss, val_correct_predictions, val_total_samples = 0.0, 0, 0 #\n",
    "            with torch.no_grad(): # Disable gradient calculations #\n",
    "                for inputs, labels in val_loader: #\n",
    "                    inputs, labels = inputs.to(device), labels.to(device) #\n",
    "                    outputs = model(inputs) # Forward pass #\n",
    "                    loss = criterion(outputs, labels) # Calculate loss #\n",
    "                    val_running_loss += loss.item() * inputs.size(0) # Accumulate loss #\n",
    "                    predicted = (outputs > 0.5).float() # Convert probs to 0/1 predictions #\n",
    "                    val_correct_predictions += (predicted == labels).sum().item() # Count correct #\n",
    "                    val_total_samples += labels.size(0) # Count total samples #\n",
    "\n",
    "            if val_total_samples > 0: #\n",
    "                epoch_val_loss = val_running_loss / val_total_samples #\n",
    "                epoch_val_acc = val_correct_predictions / val_total_samples #\n",
    "                val_losses.append(epoch_val_loss) #\n",
    "                val_accuracies.append(epoch_val_acc) #\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}] | Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.4f}') #\n",
    "\n",
    "                # --- LR Scheduler and Early Stopping ---\n",
    "                scheduler.step(epoch_val_loss) # Step scheduler based on validation loss #\n",
    "\n",
    "                if epoch_val_loss < best_val_loss: #\n",
    "                    best_val_loss = epoch_val_loss #\n",
    "                    epochs_no_improve = 0 # Reset counter #\n",
    "                    best_model_state = copy.deepcopy(model.state_dict()) # Save best model state #\n",
    "                    # print(f'  >> Validation loss improved to {best_val_loss:.4f}. Saving model state.') # Optional print\n",
    "                else:\n",
    "                    epochs_no_improve += 1 # Increment counter #\n",
    "                    # print(f'  >> Validation loss did not improve for {epochs_no_improve} epoch(s).') # Optional print\n",
    "\n",
    "                if epochs_no_improve >= patience: #\n",
    "                    print(f'\\nEarly stopping triggered after epoch {epoch + 1} due to no improvement in validation loss for {patience} epochs.') #\n",
    "                    early_stop_triggered = True; break # Stop training #\n",
    "            else: # val_loader exists but is empty\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}] | Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | Validation set is empty.') #\n",
    "                val_losses.append(None); val_accuracies.append(None) #\n",
    "                # scheduler.step(epoch_train_loss) # Optionally step on train loss if validation is empty\n",
    "        else: # No validation loader provided\n",
    "             print(f'Epoch [{epoch+1}/{num_epochs}] | Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | No validation set provided.') #\n",
    "             scheduler.step(epoch_train_loss) # Step scheduler based on training loss #\n",
    "             best_model_state = copy.deepcopy(model.state_dict()) # Save the last state if no validation #\n",
    "\n",
    "    if not early_stop_triggered: print(f'\\nTraining finished after {num_epochs} epochs.') #\n",
    "    if best_model_state is None and not early_stop_triggered and num_epochs > 0: best_model_state = model.state_dict() # Save last if training finished w/o improvement #\n",
    "\n",
    "    print(\"--- Model Training Finished ---\") #\n",
    "    actual_epochs = len(train_losses) # Get number of epochs actually run\n",
    "    # Ensure val_losses/accuracies have same length as train_losses/accuracies if early stopping occurred\n",
    "    val_losses_final = val_losses[:actual_epochs] if val_losses else []\n",
    "    val_accuracies_final = val_accuracies[:actual_epochs] if val_accuracies else []\n",
    "\n",
    "    return (train_losses, train_accuracies, val_losses_final, val_accuracies_final, best_model_state) #\n",
    "\n",
    "print(\"Training function 'train_model_with_validation' defined.\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "341c7374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function 'evaluate_model' defined.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 24: Evaluation Function Definition\n",
    "# =========================================\n",
    "# Use the function from improve.txt Cell 17/18\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device): #\n",
    "    \"\"\"Evaluates the model on a given dataset.\"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    all_preds, all_labels = [], [] #\n",
    "    eval_loss, correct_predictions, total_samples = 0.0, 0, 0 #\n",
    "    print(\"\\n--- Starting Model Evaluation ---\") #\n",
    "\n",
    "    if data_loader is None:\n",
    "        print(\"Evaluation skipped: data_loader is None.\")\n",
    "        return [], [], 0.0, 0.0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculations #\n",
    "        for inputs, labels in data_loader: #\n",
    "            inputs, labels = inputs.to(device), labels.to(device) #\n",
    "            outputs = model(inputs) # Forward pass #\n",
    "            loss = criterion(outputs, labels) # Calculate loss #\n",
    "            eval_loss += loss.item() * inputs.size(0) # Accumulate loss #\n",
    "            predicted = (outputs > 0.5).float() # Convert probs to 0/1 predictions #\n",
    "            all_preds.extend(predicted.cpu().numpy()) # Store predictions #\n",
    "            all_labels.extend(labels.cpu().numpy()) # Store true labels #\n",
    "            correct_predictions += (predicted == labels).sum().item() # Count correct #\n",
    "            total_samples += labels.size(0) # Count total samples #\n",
    "\n",
    "    if total_samples > 0: #\n",
    "        avg_eval_loss = eval_loss / total_samples #\n",
    "        accuracy = correct_predictions / total_samples #\n",
    "        print(f'Evaluation Loss: {avg_eval_loss:.4f}, Evaluation Accuracy: {accuracy:.4f}') #\n",
    "\n",
    "        all_labels_flat = np.array(all_labels).flatten(); all_preds_flat = np.array(all_preds).flatten() #\n",
    "        try:\n",
    "            print(\"\\nClassification Report:\") #\n",
    "            print(classification_report(all_labels_flat, all_preds_flat, zero_division=0)) #\n",
    "            print(\"\\nConfusion Matrix:\") #\n",
    "            print(confusion_matrix(all_labels_flat, all_preds_flat)) #\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate classification report/confusion matrix: {e}\")\n",
    "\n",
    "        print(\"--- Model Evaluation Finished ---\") #\n",
    "        return all_preds_flat, all_labels_flat, accuracy, avg_eval_loss # Return accuracy and loss\n",
    "    else:\n",
    "        print(\"Evaluation dataset is empty.\") #\n",
    "        return [], [], 0.0, 0.0 #\n",
    "\n",
    "print(\"Evaluation function 'evaluate_model' defined.\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8945e15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Training ---\n",
      "Epoch [1/50] | Train Loss: 0.6915 | Train Acc: 0.5265 | Val Loss: 0.6899 | Val Acc: 0.5335\n",
      "Epoch [2/50] | Train Loss: 0.6909 | Train Acc: 0.5276 | Val Loss: 0.6916 | Val Acc: 0.5335\n",
      "Epoch [3/50] | Train Loss: 0.6908 | Train Acc: 0.5269 | Val Loss: 0.6895 | Val Acc: 0.5335\n",
      "Epoch [4/50] | Train Loss: 0.6905 | Train Acc: 0.5279 | Val Loss: 0.6906 | Val Acc: 0.5281\n",
      "Epoch [5/50] | Train Loss: 0.6905 | Train Acc: 0.5281 | Val Loss: 0.6914 | Val Acc: 0.5316\n",
      "Epoch [6/50] | Train Loss: 0.6903 | Train Acc: 0.5278 | Val Loss: 0.6902 | Val Acc: 0.5335\n",
      "Epoch [7/50] | Train Loss: 0.6902 | Train Acc: 0.5282 | Val Loss: 0.6911 | Val Acc: 0.5335\n",
      "Epoch [8/50] | Train Loss: 0.6902 | Train Acc: 0.5276 | Val Loss: 0.6898 | Val Acc: 0.5334\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m(): \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer not defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# --- Execute Training ---\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m cnn_lstm_train_losses, cnn_lstm_train_accuracies, cnn_lstm_val_losses, cnn_lstm_val_accuracies, cnn_lstm_best_model_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_with_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcnn_lstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Handle cases where training might finish without improving (no best state saved) or if val_loader was None\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cnn_lstm_best_model_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cnn_lstm_train_losses) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# Check if training ran at all\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[85], line 26\u001b[0m, in \u001b[0;36mtrain_model_with_validation\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, patience, max_grad_norm)\u001b[0m\n\u001b[0;32m     24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs) \u001b[38;5;66;03m# Forward pass #\u001b[39;00m\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels) \u001b[38;5;66;03m# Calculate loss #\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Backward pass #\u001b[39;00m\n\u001b[0;32m     27\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39mmax_grad_norm) \u001b[38;5;66;03m# Gradient Clipping #\u001b[39;00m\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Update weights #\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Cell 25: Train the CNN-LSTM Model\n",
    "# =========================================\n",
    "# Training settings\n",
    "num_epochs = 50          # Maximum number of epochs to train\n",
    "early_stopping_patience = 10 # Number of epochs to wait for improvement before stopping\n",
    "max_grad_norm = 1.0      # Max norm for gradient clipping\n",
    "\n",
    "# Ensure loaders and scheduler are defined from previous cells\n",
    "if 'train_loader' not in locals() or train_loader is None: raise NameError(\"train_loader not defined or is None.\") #\n",
    "# val_loader can be None, handled in training function\n",
    "if 'scheduler' not in locals(): raise NameError(\"scheduler not defined.\") #\n",
    "if 'cnn_lstm_model' not in locals(): raise NameError(\"cnn_lstm_model not defined.\") #\n",
    "if 'criterion' not in locals(): raise NameError(\"criterion not defined.\") #\n",
    "if 'optimizer' not in locals(): raise NameError(\"optimizer not defined.\") #\n",
    "\n",
    "\n",
    "# --- Execute Training ---\n",
    "cnn_lstm_train_losses, cnn_lstm_train_accuracies, cnn_lstm_val_losses, cnn_lstm_val_accuracies, cnn_lstm_best_model_state = train_model_with_validation(\n",
    "    cnn_lstm_model, train_loader, val_loader, criterion, optimizer, scheduler, #\n",
    "    num_epochs, device, early_stopping_patience, max_grad_norm #\n",
    ")\n",
    "\n",
    "# Handle cases where training might finish without improving (no best state saved) or if val_loader was None\n",
    "if cnn_lstm_best_model_state is None and len(cnn_lstm_train_losses) > 0: # Check if training ran at all\n",
    "     print(\"\\nWarning: No best model state recorded (e.g., validation loss never improved or no validation set). Using final model state.\") #\n",
    "     cnn_lstm_best_model_state = cnn_lstm_model.state_dict() # Fallback to the final state #\n",
    "elif len(cnn_lstm_train_losses) == 0:\n",
    "     print(\"\\nError: Training did not run for any epochs. Cannot proceed.\")\n",
    "     # Consider raising an error or exiting depending on desired behavior\n",
    "     cnn_lstm_best_model_state = None # Ensure it's None if training failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10f5099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Cell 26: Plot Training & Validation Curves\n",
    "# =========================================\n",
    "epochs_trained = len(cnn_lstm_train_losses) # Get actual epochs trained\n",
    "if epochs_trained > 0: # Check if training actually ran\n",
    "    plt.figure(figsize=(12, 8)) #\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(2, 1, 1) #\n",
    "    plt.plot(range(1, epochs_trained + 1), cnn_lstm_train_losses, label='Training Loss', marker='.') #\n",
    "    # Check if validation loss list is not empty and contains valid numbers\n",
    "    if cnn_lstm_val_losses and any(v is not None and not np.isnan(v) for v in cnn_lstm_val_losses): #\n",
    "        plt.plot(range(1, epochs_trained + 1), [v for v in cnn_lstm_val_losses], label='Validation Loss', linestyle='--', marker='.') # Filter out None/NaN #\n",
    "    plt.title('CNN-LSTM Training & Validation Loss') #\n",
    "    plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.legend(); plt.grid(True) #\n",
    "    plt.xticks(range(1, epochs_trained + 1, max(1, epochs_trained // 10))) # Adjust x-ticks for readability\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(2, 1, 2) #\n",
    "    plt.plot(range(1, epochs_trained + 1), cnn_lstm_train_accuracies, label='Training Accuracy', marker='.') #\n",
    "    if cnn_lstm_val_accuracies and any(v is not None and not np.isnan(v) for v in cnn_lstm_val_accuracies): #\n",
    "        plt.plot(range(1, epochs_trained + 1), [v for v in cnn_lstm_val_accuracies], label='Validation Accuracy', linestyle='--', marker='.') # Filter out None/NaN #\n",
    "    plt.title('CNN-LSTM Training & Validation Accuracy') #\n",
    "    plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.legend(); plt.grid(True) #\n",
    "    plt.xticks(range(1, epochs_trained + 1, max(1, epochs_trained // 10))) # Adjust x-ticks\n",
    "\n",
    "    plt.tight_layout() #\n",
    "    plot_filename = 'cnn_lstm_training_curves.png' #\n",
    "    plt.savefig(plot_filename) #\n",
    "    print(f\"\\nTraining/Validation curves plot saved to {plot_filename}\") #\n",
    "    plt.show() #\n",
    "else:\n",
    "    print(\"Skipping plotting as no training epochs were completed.\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc75ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Cell 27: Evaluate the BEST CNN-LSTM Model on Test Set\n",
    "# =========================================\n",
    "if cnn_lstm_best_model_state: #\n",
    "    print(\"\\nLoading best CNN-LSTM model state for final test evaluation...\") #\n",
    "    cnn_lstm_model.load_state_dict(cnn_lstm_best_model_state) #\n",
    "\n",
    "    # Evaluate the loaded model on the test set\n",
    "    cnn_lstm_y_pred_test, cnn_lstm_y_true_test, cnn_lstm_test_accuracy, cnn_lstm_test_loss = evaluate_model(\n",
    "        cnn_lstm_model, test_loader, criterion, device #\n",
    "    )\n",
    "\n",
    "    if cnn_lstm_test_accuracy is not None: # Check if evaluation returned a valid accuracy #\n",
    "        cnn_lstm_test_accuracy_percentage = cnn_lstm_test_accuracy * 100 #\n",
    "        print(f\"\\nFinal Test Accuracy (using best CNN-LSTM model): {cnn_lstm_test_accuracy_percentage:.2f}%\") #\n",
    "        print(f\"Final Test Loss (using best CNN-LSTM model): {cnn_lstm_test_loss:.4f}\") #\n",
    "    else:\n",
    "        cnn_lstm_test_accuracy_percentage = 0.0 #\n",
    "        print(\"\\nCould not calculate final test accuracy (evaluation might have failed or test set empty).\") #\n",
    "else:\n",
    "    print(\"\\nError: No best CNN-LSTM model state available to evaluate (training might have failed).\") #\n",
    "    cnn_lstm_test_accuracy_percentage = 0.0 #\n",
    "    cnn_lstm_test_loss = None # Indicate loss is unavailable\n",
    "\n",
    "# Store final test accuracy for potential later comparison\n",
    "final_test_accuracy_cnn_lstm = cnn_lstm_test_accuracy_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a244679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Cell 28: Save the BEST CNN-LSTM Model\n",
    "# =========================================\n",
    "cnn_lstm_model_save_path = 'bitcoin_cnn_lstm_best_model.pth' #\n",
    "print(f\"\\nSaving BEST CNN-LSTM model state dictionary to {cnn_lstm_model_save_path}...\") #\n",
    "if cnn_lstm_best_model_state: #\n",
    "    try:\n",
    "        torch.save(cnn_lstm_best_model_state, cnn_lstm_model_save_path) #\n",
    "        print(\"Best CNN-LSTM model state saved successfully.\") #\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model state: {e}\")\n",
    "else:\n",
    "    print(\"Warning: No best CNN-LSTM model state found to save.\") #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ededd7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Cell 29: Probability & Position Generation Functions\n",
    "# =========================================\n",
    "# --- Function to get probabilities from model --- (from improve.txt Cell 23)\n",
    "def get_probabilities(model, data_loader, device): #\n",
    "    \"\"\"Generates prediction probabilities for the given data_loader.\"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    probabilities = [] #\n",
    "    print(\"\\nGenerating prediction probabilities...\") #\n",
    "    if data_loader is None:\n",
    "        print(\"Skipping probability generation: data_loader is None.\")\n",
    "        return np.array([])\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculations #\n",
    "        for inputs, _ in data_loader: # Iterate through batches\n",
    "            inputs = inputs.to(device) # Move data to device #\n",
    "            outputs = model(inputs).squeeze() # Get model outputs (squeeze removes extra dim) #\n",
    "            # Handle case where dataloader might return a single batch result not as list\n",
    "            if outputs.dim() == 0: # If scalar tensor #\n",
    "                 probabilities.append(outputs.cpu().item()) # Append single value #\n",
    "            else:\n",
    "                 probabilities.extend(outputs.cpu().numpy()) # Append batch of values #\n",
    "    print(\"Probability generation complete.\") #\n",
    "    return np.array(probabilities) # Return as numpy array #\n",
    "\n",
    "# --- Function for Fixed Threshold Positions --- (from improve.txt Cell 24)\n",
    "def get_fixed_threshold_positions(probabilities, threshold_buy=0.6, threshold_sell=0.4): # Renamed for clarity\n",
    "    \"\"\"Generates trading positions based on fixed thresholds.\"\"\"\n",
    "    if not (0 < threshold_sell < threshold_buy < 1): #\n",
    "        print(f\"Warning: Invalid fixed thresholds (buy={threshold_buy}, sell={threshold_sell}). Using defaults.\")\n",
    "        threshold_buy=0.6; threshold_sell=0.4\n",
    "    positions = np.zeros(len(probabilities)) # Initialize positions as 0 (Neutral/Hold) #\n",
    "    positions[probabilities > threshold_buy] = 1 # Set Buy signal #\n",
    "    positions[probabilities < threshold_sell] = -1 # Set Sell signal #\n",
    "    print(f\"Fixed threshold positions generated: Buy > {threshold_buy}, Sell < {threshold_sell}\") #\n",
    "    return positions #\n",
    "\n",
    "# --- Function for Dynamic Threshold Positions --- (from improve.txt Cell 24.5)\n",
    "def get_dynamic_threshold_positions(probabilities, volatility_values, base_threshold_buy=0.55, base_threshold_sell=0.45, volatility_factor=0.1, min_threshold_diff=0.05): # Renamed & added min_diff\n",
    "    \"\"\"Generates trading positions based on dynamic thresholds adjusted by volatility.\"\"\"\n",
    "    if len(probabilities) != len(volatility_values): #\n",
    "        # Try to align if off by a small amount (e.g., sequence creation edge case)\n",
    "        min_len = min(len(probabilities), len(volatility_values)) #\n",
    "        print(f\"Warning: Length mismatch in dynamic thresholds. Prob: {len(probabilities)}, Vol: {len(volatility_values)}. Aligning to {min_len} values.\") #\n",
    "        probabilities = probabilities[:min_len] #\n",
    "        volatility_values = volatility_values[:min_len] #\n",
    "\n",
    "    volatility_values = np.maximum(volatility_values, 0) # Ensure non-negative volatility #\n",
    "    # Consider normalizing volatility if its scale varies greatly, e.g., using rolling percentile\n",
    "    # vol_norm = (volatility_values - np.mean(volatility_values)) / (np.std(volatility_values) + 1e-9) # Z-score example\n",
    "\n",
    "    thresholds_buy = base_threshold_buy + volatility_values * volatility_factor #\n",
    "    thresholds_sell = base_threshold_sell - volatility_values * volatility_factor #\n",
    "\n",
    "    # Ensure buy > sell and add minimum separation\n",
    "    thresholds_buy = np.maximum(thresholds_buy, thresholds_sell + min_threshold_diff)\n",
    "\n",
    "    # Clip thresholds to valid probability ranges (e.g., 0.05 to 0.95)\n",
    "    thresholds_buy = np.clip(thresholds_buy, 0.5 + min_threshold_diff / 2, 0.95) # Ensure buy > 0.5 #\n",
    "    thresholds_sell = np.clip(thresholds_sell, 0.05, 0.5 - min_threshold_diff / 2) # Ensure sell < 0.5 #\n",
    "\n",
    "    positions = np.zeros(len(probabilities)) # Initialize positions #\n",
    "    positions[probabilities > thresholds_buy] = 1  # Buy signal\n",
    "    positions[probabilities < thresholds_sell] = -1 # Sell signal\n",
    "\n",
    "    # for i in range(len(probabilities)): # Original loop for debugging if needed\n",
    "    #     if probabilities[i] > thresholds_buy[i]: positions[i] = 1\n",
    "    #     elif probabilities[i] < thresholds_sell[i]: positions[i] = -1\n",
    "\n",
    "    print(f\"Dynamic threshold positions generated using volatility factor {volatility_factor}.\") #\n",
    "    # print(f\"Example thresholds (first 5): Buy={thresholds_buy[:5]}, Sell={thresholds_sell[:5]}\") # Optional: Print example thresholds\n",
    "    return positions #\n",
    "\n",
    "print(\"Probability and position generation functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ae14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Cell 30: Stop Loss Function Definition\n",
    "# =========================================\n",
    "# Use the function from improve.txt Cell 26.5\n",
    "\n",
    "def implement_stop_loss(positions, prices, stop_percent=0.02): #\n",
    "    \"\"\"Applies a simple stop-loss logic to trading positions.\"\"\"\n",
    "    print(f\"Applying stop loss simulation with stop_percent={stop_percent:.2%}...\") #\n",
    "    if len(positions) != len(prices): #\n",
    "         min_len = min(len(positions), len(prices)) #\n",
    "         print(f\"Warning: Length mismatch in stop loss. Pos: {len(positions)}, Prices: {len(prices)}. Aligning to {min_len} values.\") #\n",
    "         positions = positions[:min_len] #\n",
    "         prices = prices[:min_len] #\n",
    "\n",
    "    positions_sl = np.copy(positions) # Work on a copy to preserve original signals #\n",
    "    active_pos_type = 0      # Current state: 0=flat, 1=long, -1=short\n",
    "    entry_price = 0.0        # Price at which the current position was entered\n",
    "    stop_loss_price = 0.0    # Price level at which to exit the current position\n",
    "\n",
    "    for i in range(len(prices)): # Iterate through each time step\n",
    "        current_price = prices[i] #\n",
    "        original_signal = positions[i] # Signal for *this* time step from the model\n",
    "\n",
    "        # --- 1. Check if stop loss was hit based on the *previous* state ---\n",
    "        stop_hit = False #\n",
    "        if active_pos_type == 1 and current_price < stop_loss_price: # Check long stop loss #\n",
    "            stop_hit = True #\n",
    "            # print(f\"Stop Loss HIT LONG at index {i}, Price: {current_price:.2f}, SL: {stop_loss_price:.2f}\") # Debug print\n",
    "        elif active_pos_type == -1 and current_price > stop_loss_price: # Check short stop loss #\n",
    "            stop_hit = True #\n",
    "            # print(f\"Stop Loss HIT SHORT at index {i}, Price: {current_price:.2f}, SL: {stop_loss_price:.2f}\") # Debug print\n",
    "\n",
    "        if stop_hit:\n",
    "            active_pos_type = 0 # Exit position #\n",
    "            positions_sl[i] = 0 # Force exit signal *at this point* due to SL #\n",
    "            entry_price = 0.0 # Reset entry price #\n",
    "            stop_loss_price = 0.0 # Reset SL price #\n",
    "            # NOTE: This simple version allows immediate re-entry if the original signal persists.\n",
    "            # More complex logic could enforce a waiting period after a stop-out.\n",
    "\n",
    "        # --- 2. Determine action based on the original signal, considering current state ---\n",
    "        if active_pos_type == 0: # If currently flat\n",
    "            if original_signal != 0: # Signal to enter a new position\n",
    "                active_pos_type = original_signal # Set new position type (1 or -1) #\n",
    "                entry_price = current_price # Record entry price #\n",
    "                # Calculate stop loss price based on entry\n",
    "                stop_loss_price = entry_price * (1 - stop_percent * active_pos_type) # Correct calculation for long/short #\n",
    "                positions_sl[i] = active_pos_type # Reflect the entry in the SL-adjusted signals #\n",
    "                # print(f\"Entry {'Long' if active_pos_type==1 else 'Short'} at {i}, Price: {entry_price:.2f}, SL: {stop_loss_price:.2f}\") # Debug print\n",
    "            # else: remain flat (original_signal is 0), positions_sl[i] is already 0 unless SL hit\n",
    "\n",
    "        else: # If already in a position (long or short)\n",
    "            if original_signal == 0 or original_signal == -active_pos_type: # Signal to exit or reverse\n",
    "                 # print(f\"Exit/Reverse {'Long' if active_pos_type==1 else 'Short'} at {i}, Signal: {original_signal}, Price: {current_price:.2f}\") # Debug print\n",
    "                exit_pos_type = active_pos_type # Store type before changing\n",
    "                active_pos_type = original_signal # Set new state (0 for exit, -1 or 1 for reversal) #\n",
    "                positions_sl[i] = active_pos_type # Reflect exit/reversal in SL signals #\n",
    "\n",
    "                if active_pos_type != 0: # If it was a reversal (not just exit to flat)\n",
    "                    entry_price = current_price # Set new entry price for the reversed position #\n",
    "                    stop_loss_price = entry_price * (1 - stop_percent * active_pos_type) # Calculate new SL #\n",
    "                    # print(f\"Reversal Entry {'Long' if active_pos_type==1 else 'Short'} at {i}, Price: {entry_price:.2f}, SL: {stop_loss_price:.2f}\") # Debug print\n",
    "                else: # Exited to flat\n",
    "                    entry_price = 0.0; stop_loss_price = 0.0 # Reset entry/SL prices #\n",
    "\n",
    "            else: # Signal confirms holding the existing position (original_signal == active_pos_type)\n",
    "                 positions_sl[i] = active_pos_type # Maintain the position signal #\n",
    "                 # Optional: Implement trailing stop loss here if desired\n",
    "                 # if active_pos_type == 1: stop_loss_price = max(stop_loss_price, current_price * (1-stop_percent))\n",
    "                 # elif active_pos_type == -1: stop_loss_price = min(stop_loss_price, current_price * (1+stop_percent))\n",
    "\n",
    "    print(\"Stop loss simulation applied.\") #\n",
    "    return positions_sl # Return the signals adjusted for stop losses #\n",
    "\n",
    "print(\"Stop loss function 'implement_stop_loss' defined.\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7859284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Cell 31: Generate Predictions & Signals for Test Set using CNN-LSTM\n",
    "# =========================================\n",
    "# Ensure the best model state is loaded\n",
    "if cnn_lstm_best_model_state: #\n",
    "    cnn_lstm_model.load_state_dict(cnn_lstm_best_model_state) #\n",
    "    print(\"Best CNN-LSTM model state loaded for prediction.\")\n",
    "elif 'cnn_lstm_model' in locals():\n",
    "    print(\"Warning: Best CNN-LSTM model state not found. Using the current model state for prediction.\") #\n",
    "else:\n",
    "    raise NameError(\"CNN-LSTM model not available for prediction.\")\n",
    "\n",
    "# Generate probabilities on the test set\n",
    "cnn_lstm_y_pred_proba_test = get_probabilities(cnn_lstm_model, test_loader, device) #\n",
    "\n",
    "# --- Choose Signal Generation Method ---\n",
    "use_dynamic_thresholds = True # Set to True to use dynamic, False for fixed\n",
    "volatility_col_for_thresholds = 'ATR' # Column to use for dynamic thresholds (e.g., 'ATR', 'Volatility_14d')\n",
    "base_buy = 0.55\n",
    "base_sell = 0.45\n",
    "vol_factor = 0.1\n",
    "\n",
    "# Align volatility metric with test predictions\n",
    "# Need the original dataframe 'df' and split indices from Cell 15\n",
    "# test_start_index_in_df = final_train_size + val_size + time_steps # Start index in ORIGINAL df for test targets\n",
    "# test_end_index_in_df = test_start_index_in_df + len(y_test_seq) # End index\n",
    "\n",
    "# Recalculate indices based on lengths from Cell 15 and time_steps from Cell 17\n",
    "# Note: The split was on X_scaled, length len(X_scaled). Sequences start 'time_steps' later.\n",
    "# The targets y_test_seq correspond to predictions cnn_lstm_y_pred_proba_test\n",
    "# We need the volatility from the time points corresponding to y_test_seq\n",
    "\n",
    "test_target_start_index_in_df = final_train_size + val_size + time_steps # Index in original df where test targets BEGIN\n",
    "test_target_end_index_in_df = final_train_size + val_size + test_size # Index in original df where test targets END (exclusive)\n",
    "\n",
    "if len(cnn_lstm_y_pred_proba_test) != (test_target_end_index_in_df - test_target_start_index_in_df):\n",
    "    print(f\"Warning: Length mismatch between predictions ({len(cnn_lstm_y_pred_proba_test)}) and expected target slice size ({test_target_end_index_in_df - test_target_start_index_in_df}). Check split/sequence logic.\")\n",
    "    # Adjust end index if lengths misalign, but this might indicate an earlier issue\n",
    "    test_target_end_index_in_df = test_target_start_index_in_df + len(cnn_lstm_y_pred_proba_test)\n",
    "\n",
    "\n",
    "if use_dynamic_thresholds and volatility_col_for_thresholds in df.columns: #\n",
    "    # Extract volatility values corresponding to the test set predictions\n",
    "    test_volatility = df[volatility_col_for_thresholds].iloc[test_target_start_index_in_df : test_target_end_index_in_df].values #\n",
    "    test_volatility = np.nan_to_num(test_volatility, nan=np.nanmean(test_volatility)) # Handle potential NaNs #\n",
    "\n",
    "    print(f\"\\nUsing Dynamic Thresholds based on '{volatility_col_for_thresholds}'. Shape: {test_volatility.shape}\") #\n",
    "    if len(cnn_lstm_y_pred_proba_test) == len(test_volatility): # Check alignment again\n",
    "         cnn_lstm_test_positions = get_dynamic_threshold_positions( #\n",
    "             cnn_lstm_y_pred_proba_test, test_volatility, #\n",
    "             base_threshold_buy=base_buy, base_threshold_sell=base_sell, volatility_factor=vol_factor # Adjust params\n",
    "         ) #\n",
    "    else:\n",
    "         print(f\"Error: Length mismatch after extracting volatility Prob({len(cnn_lstm_y_pred_proba_test)}) vs Vol({len(test_volatility)}). Falling back to fixed thresholds.\") #\n",
    "         cnn_lstm_test_positions = get_fixed_threshold_positions(cnn_lstm_y_pred_proba_test, threshold_buy=base_buy, threshold_sell=base_sell) # Fallback #\n",
    "else:\n",
    "    if use_dynamic_thresholds: print(f\"\\nWarning: Volatility column '{volatility_col_for_thresholds}' not found. Using fixed thresholds.\") #\n",
    "    else: print(\"\\nUsing Fixed Thresholds.\")\n",
    "    cnn_lstm_test_positions = get_fixed_threshold_positions(cnn_lstm_y_pred_proba_test, threshold_buy=base_buy, threshold_sell=base_sell) # Fallback or intended fixed #\n",
    "\n",
    "print(f\"Generated {len(cnn_lstm_test_positions)} initial trading positions for CNN-LSTM.\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1f6080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Cell 32: Prepare Data for Backtest\n",
    "# =========================================\n",
    "# Get dates and prices corresponding to the prediction targets/positions\n",
    "# Use the same indices calculated in Cell 31\n",
    "test_dates = df.index[test_target_start_index_in_df : test_target_end_index_in_df] #\n",
    "test_prices = df[price_col].iloc[test_target_start_index_in_df : test_target_end_index_in_df].values # Use the primary price column #\n",
    "\n",
    "# Ensure lengths match before stop loss / backtest\n",
    "min_len_backtest = min(len(test_dates), len(test_prices), len(cnn_lstm_test_positions)) #\n",
    "if len(test_dates) != min_len_backtest or len(test_prices) != min_len_backtest or len(cnn_lstm_test_positions) != min_len_backtest: #\n",
    "     print(f\"Warning: Aligning dates/prices/positions for backtest to minimum length: {min_len_backtest}\") #\n",
    "     test_dates = test_dates[:min_len_backtest] #\n",
    "     test_prices = test_prices[:min_len_backtest] #\n",
    "     cnn_lstm_test_positions = cnn_lstm_test_positions[:min_len_backtest] #\n",
    "\n",
    "print(f\"\\nAligned dates, prices, and positions for backtesting. Length: {len(cnn_lstm_test_positions)}\") #\n",
    "if len(cnn_lstm_test_positions) == 0:\n",
    "    raise ValueError(\"No positions available for backtesting. Check data alignment and prediction steps.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b008f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Cell 33: Apply Stop Loss Simulation to CNN-LSTM Positions\n",
    "# =========================================\n",
    "stop_loss_percentage = 0.02 # Example: 2% stop loss\n",
    "\n",
    "cnn_lstm_test_positions_with_sl = implement_stop_loss( #\n",
    "    cnn_lstm_test_positions, test_prices, stop_percent=stop_loss_percentage #\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(cnn_lstm_test_positions_with_sl)} positions after stop loss simulation.\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5afec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Cell 34: Run Backtest Simulation for CNN-LSTM\n",
    "# =========================================\n",
    "print(\"\\nCalculating CNN-LSTM strategy returns using positions WITH stop loss...\") #\n",
    "# Use the positions modified by the stop loss function\n",
    "cnn_lstm_strategy_returns = np.zeros(len(cnn_lstm_test_positions_with_sl)) # Initialize returns array #\n",
    "cnn_lstm_trade_count = 0 # Initialize trade counter #\n",
    "\n",
    "for i in range(1, len(cnn_lstm_test_positions_with_sl)): # Start from second time step\n",
    "    position_held = cnn_lstm_test_positions_with_sl[i-1] # Position decided based on signal BEFORE period i starts #\n",
    "    price_prev = test_prices[i-1] # Price at the start of period i\n",
    "    price_curr = test_prices[i] # Price at the end of period i\n",
    "\n",
    "    if price_prev == 0: continue # Skip if previous price is zero to avoid division errors #\n",
    "\n",
    "    # Detect trades (change in position) for counting\n",
    "    if cnn_lstm_test_positions_with_sl[i] != cnn_lstm_test_positions_with_sl[i-1]: #\n",
    "        cnn_lstm_trade_count += 1 #\n",
    "\n",
    "    # Calculate return based on the position held *during* period i\n",
    "    if position_held == 1: # If held long position\n",
    "        cnn_lstm_strategy_returns[i] = (price_curr / price_prev) - 1 # Long return #\n",
    "    elif position_held == -1: # If held short position\n",
    "        cnn_lstm_strategy_returns[i] = (price_prev / price_curr) - 1 # Short return (inverted price ratio) #\n",
    "    # else: position_held == 0, return is 0 (already initialized)\n",
    "\n",
    "# Calculate cumulative returns\n",
    "# Add 1 for cumulative product, subtract 1 at the end\n",
    "cnn_lstm_strategy_cumulative_returns = (1 + cnn_lstm_strategy_returns).cumprod() - 1 #\n",
    "\n",
    "# Calculate Buy-and-Hold returns for comparison using test_prices\n",
    "if len(test_prices) > 0 and test_prices[0] != 0:\n",
    "    buy_hold_returns = (test_prices / test_prices[0]) - 1 #\n",
    "else:\n",
    "    buy_hold_returns = np.zeros(len(test_prices)) # Avoid division by zero if first price is 0\n",
    "\n",
    "print(\"CNN-LSTM return calculations complete.\") #\n",
    "print(f\"Approximate CNN-LSTM Trade Count (based on SL-modified signals): {cnn_lstm_trade_count}\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e45d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Cell 35: Plot CNN-LSTM Backtest Results\n",
    "# =========================================\n",
    "plt.figure(figsize=(14, 7)) #\n",
    "\n",
    "# Plot CNN-LSTM Strategy Returns\n",
    "plt.plot(test_dates, cnn_lstm_strategy_cumulative_returns * 100,\n",
    "         label=f'CNN-LSTM Strategy (w/ SL {stop_loss_percentage*100:.0f}%)', color='red', linewidth=2) #\n",
    "\n",
    "# Plot Buy and Hold Returns\n",
    "plt.plot(test_dates, buy_hold_returns * 100, label='Buy and Hold', alpha=0.7, color='blue', linestyle='--') #\n",
    "\n",
    "# --- Optional: Add Original LSTM results for comparison ---\n",
    "# Check if original LSTM results exist from a previous run/notebook part\n",
    "# Assuming 'strategy_cumulative_returns' holds original LSTM results\n",
    "# if 'strategy_cumulative_returns' in locals() and len(strategy_cumulative_returns) == len(test_dates):\n",
    "#     plt.plot(test_dates, strategy_cumulative_returns * 100,\n",
    "#              label=f'Original LSTM Strategy (w/ SL {stop_loss_percentage*100:.0f}%)',\n",
    "#              color='green', linestyle=':')\n",
    "\n",
    "plt.title('CNN-LSTM Strategy vs Buy and Hold (Test Set)') #\n",
    "plt.xlabel('Date') #\n",
    "plt.ylabel('Cumulative Returns (%)') #\n",
    "plt.legend() #\n",
    "plt.grid(True) #\n",
    "plt.tight_layout() #\n",
    "\n",
    "# Save the plot\n",
    "cnn_lstm_perf_plot_filename = 'cnn_lstm_strategy_performance.png' #\n",
    "plt.savefig(cnn_lstm_perf_plot_filename) #\n",
    "print(f\"\\nCNN-LSTM strategy performance plot saved to {cnn_lstm_perf_plot_filename}\") #\n",
    "plt.show() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ff8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Cell 36: Display CNN-LSTM Backtest Statistics\n",
    "# =========================================\n",
    "cnn_lstm_final_return = cnn_lstm_strategy_cumulative_returns[-1] * 100 if len(cnn_lstm_strategy_cumulative_returns) > 0 else 0 #\n",
    "buyhold_final_return = buy_hold_returns[-1] * 100 if len(buy_hold_returns) > 0 else 0 #\n",
    "\n",
    "# Optional: Get original LSTM final return if available\n",
    "# original_lstm_final_return = strategy_cumulative_returns[-1] * 100 if 'strategy_cumulative_returns' in locals() and len(strategy_cumulative_returns) > 0 else 0\n",
    "\n",
    "print(\"\\n--- CNN-LSTM Backtest Summary ---\") #\n",
    "if len(test_dates) > 0: #\n",
    "    print(f\"Test Period Start Date: {test_dates[0].strftime('%Y-%m-%d')}\") # Format date\n",
    "    print(f\"Test Period End Date:   {test_dates[-1].strftime('%Y-%m-%d')}\") # Format date\n",
    "else:\n",
    "    print(\"Test Period: Empty\") #\n",
    "\n",
    "print(\"\\nFinal Cumulative Returns:\")\n",
    "print(f\"CNN-LSTM Strategy: {cnn_lstm_final_return:.2f}% (Using SL-modified signals)\") #\n",
    "# print(f\"Original LSTM:     {original_lstm_final_return:.2f}% (If compared)\") # Optional comparison\n",
    "print(f\"Buy and Hold:      {buyhold_final_return:.2f}%\") #\n",
    "\n",
    "# Signal distribution comparison (Using SL-modified signals)\n",
    "cnn_lstm_num_buy = np.sum(cnn_lstm_test_positions_with_sl == 1) #\n",
    "cnn_lstm_num_sell = np.sum(cnn_lstm_test_positions_with_sl == -1) #\n",
    "cnn_lstm_num_neutral = np.sum(cnn_lstm_test_positions_with_sl == 0) #\n",
    "cnn_lstm_total_signals = len(cnn_lstm_test_positions_with_sl) #\n",
    "\n",
    "print(\"\\nCNN-LSTM Signal Distribution (After Stop Loss):\") #\n",
    "if cnn_lstm_total_signals > 0: #\n",
    "    print(f\" - Total Time Long:    {cnn_lstm_num_buy} ({cnn_lstm_num_buy/cnn_lstm_total_signals*100:.1f}%)\") #\n",
    "    print(f\" - Total Time Short:   {cnn_lstm_num_sell} ({cnn_lstm_num_sell/cnn_lstm_total_signals*100:.1f}%)\") #\n",
    "    print(f\" - Total Time Neutral: {cnn_lstm_num_neutral} ({cnn_lstm_num_neutral/cnn_lstm_total_signals*100:.1f}%)\") #\n",
    "else:\n",
    "    print(\"No CNN-LSTM signals generated for distribution analysis.\") #\n",
    "\n",
    "# Compare test accuracy (Calculated in Cell 27)\n",
    "print(\"\\nModel Accuracy on Test Set:\")\n",
    "print(f\"CNN-LSTM Test Accuracy: {final_test_accuracy_cnn_lstm:.2f}%\") # Use stored variable #\n",
    "# print(f\"Original LSTM Test Accuracy: {test_accuracy_percentage:.2f}%\") # Optional: From original model's run\n",
    "\n",
    "# Add Trade Count\n",
    "print(f\"\\nApproximate Trade Count: {cnn_lstm_trade_count}\") #\n",
    "\n",
    "print(\"--------------------------------\") #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fae7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Cell 37: Prediction Function Definition for New Data (CNN-LSTM)\n",
    "# =========================================\n",
    "# Based on improve.txt Cell 31\n",
    "\n",
    "def predict_trade_signal_cnn_lstm(new_data_df, model, scaler, feature_columns, time_steps, device,\n",
    "                                  threshold_buy=0.55, threshold_sell=0.45, # Default thresholds\n",
    "                                  use_dynamic=True, vol_col='ATR', vol_factor=0.1, min_threshold_diff=0.05): # Allow configuring prediction thresholds\n",
    "    \"\"\"Predicts the next trading signal using the trained CNN-LSTM model on new data.\"\"\"\n",
    "    print(\"\\n--- Predicting on New Data with CNN-LSTM Model ---\") #\n",
    "\n",
    "    # 1. Input Validation\n",
    "    if len(new_data_df) < time_steps: #\n",
    "        print(f\"Error: Need at least {time_steps} rows for sequence, got {len(new_data_df)}.\") #\n",
    "        return None, None # Return None for probability and signal\n",
    "\n",
    "    missing_cols = [col for col in feature_columns if col not in new_data_df.columns] #\n",
    "    if missing_cols: #\n",
    "        print(f\"Error: New data is missing required feature columns: {missing_cols}\") #\n",
    "        return None, None #\n",
    "\n",
    "    # 2. Prepare Input Data\n",
    "    # Select the last 'time_steps' rows and the required features\n",
    "    X_new = new_data_df[feature_columns].iloc[-time_steps:].values #\n",
    "    if np.isnan(X_new).any(): # Check for NaNs in the final input sequence #\n",
    "        print(f\"Error: NaNs detected in the last {time_steps} rows of the selected features. Cannot predict.\") #\n",
    "        return None, None #\n",
    "\n",
    "    # 3. Scale Data\n",
    "    # Use the *same* scaler that was fitted on the training data\n",
    "    X_new_scaled = scaler.transform(X_new) # Use transform, NOT fit_transform #\n",
    "\n",
    "    # 4. Create Sequence and Tensor\n",
    "    # Reshape for model input: (1, time_steps, n_features)\n",
    "    X_new_seq = np.array([X_new_scaled]) # Add batch dimension #\n",
    "    X_new_tensor = torch.FloatTensor(X_new_seq).to(device) # Convert to tensor and move to device #\n",
    "\n",
    "    # 5. Make Prediction\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad(): # Disable gradients for prediction\n",
    "        probability = model(X_new_tensor).squeeze().cpu().item() # Get probability output #\n",
    "\n",
    "    # 6. Determine Signal (Dynamic or Fixed)\n",
    "    signal = 0 # Default to neutral\n",
    "    if use_dynamic: #\n",
    "        if vol_col in new_data_df.columns: #\n",
    "            # Get the *last* volatility value from the input data frame\n",
    "            last_volatility = new_data_df[vol_col].iloc[-1] #\n",
    "            last_volatility = np.nan_to_num(last_volatility, nan=0.0) # Handle potential NaN in volatility #\n",
    "            last_volatility = max(0, last_volatility) # Ensure non-negative\n",
    "\n",
    "            # Calculate dynamic thresholds\n",
    "            dynamic_buy = threshold_buy + last_volatility * vol_factor #\n",
    "            dynamic_sell = threshold_sell - last_volatility * vol_factor #\n",
    "\n",
    "            # Ensure buy > sell and clip\n",
    "            dynamic_buy = max(dynamic_buy, dynamic_sell + min_threshold_diff)\n",
    "            dynamic_buy = np.clip(dynamic_buy, 0.5 + min_threshold_diff / 2, 0.95) #\n",
    "            dynamic_sell = np.clip(dynamic_sell, 0.05, 0.5 - min_threshold_diff / 2) #\n",
    "\n",
    "            print(f\"Using dynamic thresholds based on last '{vol_col}' ({last_volatility:.4f}): Buy > {dynamic_buy:.3f}, Sell < {dynamic_sell:.3f}\") #\n",
    "            if probability > dynamic_buy: signal = 1 #\n",
    "            elif probability < dynamic_sell: signal = -1 #\n",
    "            else: signal = 0 #\n",
    "\n",
    "        else: # Fallback to fixed if volatility column not found\n",
    "            print(f\"Warning: Volatility column '{vol_col}' not found for dynamic threshold. Using fixed.\") #\n",
    "            if probability > threshold_buy: signal = 1 #\n",
    "            elif probability < threshold_sell: signal = -1 #\n",
    "            else: signal = 0 #\n",
    "    else: # Use fixed thresholds\n",
    "        print(f\"Using fixed thresholds: Buy > {threshold_buy}, Sell < {threshold_sell}\") #\n",
    "        if probability > threshold_buy: signal = 1 #\n",
    "        elif probability < threshold_sell: signal = -1 #\n",
    "        else: signal = 0 #\n",
    "\n",
    "    print(f\"CNN-LSTM Prediction for next timestep: Probability={probability:.4f}, Signal={signal}\") #\n",
    "    print(\"-----------------------------\") #\n",
    "    return probability, signal # Return both probability and the final signal #\n",
    "\n",
    "print(\"Prediction function 'predict_trade_signal_cnn_lstm' defined.\") #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68175af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Cell 38: Example Usage for New Data Prediction using CNN-LSTM\n",
    "# =========================================\n",
    "print(\"\\n--- Example: Using the CNN-LSTM model for a new prediction ---\") #\n",
    "\n",
    "# This cell demonstrates how to use the prediction function.\n",
    "# In a real scenario, you would fetch *new* data and preprocess it *exactly*\n",
    "# as the training data was processed (Cells 5-12).\n",
    "\n",
    "# For demonstration, we use the last `time_steps` rows from the fully processed `df`.\n",
    "if 'df' in locals() and len(df) >= time_steps and 'cnn_lstm_model' in locals() and 'scaler' in locals() and 'feature_columns' in locals(): # Check required vars exist #\n",
    "    demo_new_data = df.iloc[-time_steps:].copy() # Use last processed rows for demo #\n",
    "    print(f\"Using last {time_steps} rows of processed data (shape: {demo_new_data.shape}) for demo prediction.\") #\n",
    "\n",
    "    # Call the prediction function\n",
    "    cnn_lstm_probability, cnn_lstm_signal = predict_trade_signal_cnn_lstm(\n",
    "        new_data_df=demo_new_data,        # The dataframe slice (must be fully preprocessed)\n",
    "        model=cnn_lstm_model,             # The trained CNN-LSTM model\n",
    "        scaler=scaler,                    # The scaler fitted on training data\n",
    "        feature_columns=feature_columns,  # List of features the model expects\n",
    "        time_steps=time_steps,            # Lookback window size\n",
    "        device=device,                    # CPU or GPU device\n",
    "        threshold_buy=0.55,               # Base threshold for dynamic or fixed buy\n",
    "        threshold_sell=0.45,              # Base threshold for dynamic or fixed sell\n",
    "        use_dynamic=True,                 # Use dynamic thresholds?\n",
    "        vol_col='ATR',                    # Volatility column for dynamic thresholds\n",
    "        vol_factor=0.1                    # Factor to adjust thresholds by volatility\n",
    "    ) #\n",
    "\n",
    "    # Interpret the predicted signal\n",
    "    if cnn_lstm_signal is not None: # Check if prediction was successful\n",
    "        print(\"\\nCNN-LSTM Predicted Trading Action for the NEXT time step:\") #\n",
    "        if cnn_lstm_signal == 1: #\n",
    "            print(\"  >> BUY\") #\n",
    "        elif cnn_lstm_signal == -1: #\n",
    "            print(\"  >> SELL\") #\n",
    "        else: # cnn_lstm_signal == 0\n",
    "            print(\"  >> HOLD / NEUTRAL\") #\n",
    "    else:\n",
    "        print(\"\\nCNN-LSTM prediction failed (check errors above).\") #\n",
    "else:\n",
    "    print(\"\\nCannot run prediction example: Check if 'df', 'cnn_lstm_model', 'scaler', 'feature_columns', 'time_steps' are available and 'df' has enough rows.\") #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e67572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Cell 39: Simple Trading Strategy Guide Summary\n",
    "# =========================================\n",
    "print(\"\\n--- Simple Bitcoin Trading Strategy Guide (CNN-LSTM Model) ---\") #\n",
    "print(f\"1. Model Type: Convolutional Neural Network (CNN) + Long Short-Term Memory (LSTM) with Attention.\") #\n",
    "print(f\"2. Input Features ({len(feature_columns)}): Includes OHLCV, technical indicators (MAs, MACD, RSI, BB, Stoch, ATR), volume metrics, volatility, trend indicators, and potentially flow/on-chain proxies.\") #\n",
    "print(f\"3. Lookback Window: Uses the previous {time_steps} time steps to predict the next step.\") #\n",
    "print(f\"4. Training: Includes Validation Set, Early Stopping (patience={early_stopping_patience}), LR Scheduling, Gradient Clipping. Optional SMOTE balancing.\") #\n",
    "print(f\"5. Signal Generation: Uses {'Dynamic thresholds based on ' + volatility_col_for_thresholds if use_dynamic_thresholds else 'Fixed thresholds'} (Base Buy > {base_buy}, Base Sell < {base_sell}).\") #\n",
    "print(f\"6. Backtest Simulation: Includes a {stop_loss_percentage*100:.0f}% stop loss.\") #\n",
    "print(f\"7. Best Model's Test Accuracy: {final_test_accuracy_cnn_lstm:.2f}%\") # Use stored variable #\n",
    "print(\"\\nDisclaimer: This is an educational model. Past performance is not indicative of future results. Trading involves significant risk.\") #\n",
    "print(\"-----------------------------------------\") #\n",
    "print(\"\\n====================================================\")\n",
    "print(\"CNN-LSTM Model Pipeline Execution Complete!\")\n",
    "print(\"====================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
